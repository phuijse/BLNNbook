{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (brief) tutorial on [Pyro](https://pyro.ai/)\n",
    "\n",
    "Pyro can be used to perform MCMC and/or approximate inference for intractable posteriors\n",
    "\n",
    "We can use Pyro to move from point estimates to posteriors in our **torch-based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "display(pyro.__version__)\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a normal distribution\n",
    "\n",
    "Distributions in Pyro are implemented in [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "The `Normal` object expects location $\\mu$ and scale $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal\n",
    "\n",
    "w_prior = Normal(loc=torch.tensor(0.), \n",
    "                 scale=torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1000 samples \n",
    "samples = w_prior.rsample(sample_shape=(1000, ))\n",
    "display(samples.shape)\n",
    "# Build an histogram\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(samples.detach().numpy(), bins=20, density=True)\n",
    "# Plot the pdf\n",
    "w_plot = np.linspace(-3, 3, num=100)\n",
    "w_pdf = torch.exp(w_prior.log_prob(torch.from_numpy(w_plot))).detach().numpy()\n",
    "plt.plot(w_plot, w_pdf, 'k-', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean\n",
    "display(w_prior.mean)\n",
    "#standard deviation\n",
    "display(w_prior.stddev)\n",
    "#entropy\n",
    "display(w_prior.entropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of pyro tensors\n",
    "\n",
    "A distribution has two shapes\n",
    "\n",
    "`event_shape` refers to the dimensionality of the distribution, e.g. normal (number), multivariate normal (vector), Cholesky (matrix), etc\n",
    "\n",
    "> `event_shape` denotes dependent random variables\n",
    "\n",
    "`batch_shape` refers to a batch of distributions\n",
    "\n",
    "> `batch_shape` denotes conditionally independent random variables (typically our data dimension)\n",
    "\n",
    "We can create a batched distribution by batching the parameters\n",
    "\n",
    "The shape of a sampled tensor will be the sum of event and batch shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two conditionally independent normal RVs\n",
    "w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]]))\n",
    "# A multivariate normal with diagonal covariance\n",
    "#w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]])).to_event(1)\n",
    "\n",
    "display(w_prior.batch_shape)\n",
    "display(w_prior.event_shape)\n",
    "display(w_prior.rsample().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables in Pyro\n",
    "\n",
    "To create random variables that we can track within a model we use [`pyro.sample`](http://pyro.ai/examples/intro_part_i.html#The-pyro.sample-Primitive)\n",
    "\n",
    "`sample` expects a name and an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "For example, to create a variable named \"w\" with the previously defined distribution\n",
    "$$\n",
    "\\begin{align}\n",
    "w \\sim &\\mathcal{N}(\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&\\mu = \\begin{pmatrix}0 \\\\ 2 \\end{pmatrix}, \\sigma = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    return pyro.sample(name='w', fn=w_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we run a model a random sample is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and conditioning \n",
    "\n",
    "Let's consider a linear regression model\n",
    "\n",
    "$$\n",
    "y_i = w x_i + b, \\forall i\n",
    "$$\n",
    "\n",
    "We will write this model in Pyro \n",
    "\n",
    "For this we consider\n",
    "- $w$ and $b$ to be random variables with normal distributions (priors)\n",
    "- $y$ to be a random variable with normal distribution (likelihood)\n",
    "- $x$ to be a deterministic variable\n",
    "- $y$ is continioned to the observed data $\\{y_i\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.0))\n",
    "\n",
    "def conditioned_model(x, y):\n",
    "    return pyro.condition(model, data={\"y\": y})(x)\n",
    "\n",
    "# or equivalently\n",
    "\n",
    "def model_obs(x, y=None):  # equivalent to conditioned_scale above\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.0), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical details\n",
    "\n",
    "To create conditions we can use `pyro.condition` or the `obs` keyword of `pyro.sample`\n",
    "\n",
    "To create conditions on the whole dataset (assuming iid) we use [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate), which expects a name and the size of the dataset\n",
    "\n",
    "In this case we use `pyro.plate` as a context (vectorized plate), it can also be used as an iterator\n"
   ]
  },
  {
   "attachments": {
    "graphical_model.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorical details\n",
    "\n",
    "In summary the model has to define the generative process \n",
    "\n",
    "In this case\n",
    "- Choose hyperparameters: $\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\sigma_\\epsilon$\n",
    "- Sample: $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$\n",
    "- Sample: $b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$\n",
    "- For each $i=1,2,\\ldots, N$\n",
    "    - Sample: $y_i \\sim \\mathcal{N}(w x_i + b, \\sigma_\\epsilon^2)$\n",
    "\n",
    "This is often summarized using plate notation diagrams\n",
    "\n",
    "![graphical_model.png](attachment:graphical_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now that we have specified the model we would like to obtain the posterior of the parameters given the data and do predictions\n",
    "\n",
    "> If we can not solve it analytically or via enumeration we can resort to approximate inference\n",
    "\n",
    "Pyro offers different ways to perform approximate inference in the module [`pyro.infer`](https://docs.pyro.ai/en/stable/inference.html)\n",
    "\n",
    "For now we will focus on **Stochastic Variational Inference** \n",
    "\n",
    "\n",
    "The unified Variational Inference interface in Pyro is located in [`pyro.infer.SVI`](http://docs.pyro.ai/en/stable/inference_algos.html) \n",
    "\n",
    "To use SVI we need to specify\n",
    "1. A model function that defines our generative model\n",
    "1. A guide function that defines our approximate posterior\n",
    "1. A cost function \n",
    "1. An optimizer\n",
    "1. Number of samples to compute Monte-Carlo estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide function\n",
    "\n",
    "> The guide represents our approximate posterior $q_\\nu(\\theta)$ \n",
    "\n",
    "The guide has to define the distribution of the posterior of the parameters \n",
    "\n",
    "We use [`pyro.param`]() to register the hyperparameters of the approximate posterior $\\eta$\n",
    "\n",
    "> These parameters are the ones that we learn through optimization\n",
    "\n",
    "**Technical detail:** The guide function has the same inputs as the model function\n",
    "\n",
    "In this example we set a normal posterior $\\theta=(w, b)$ and create the corresponding hyperparameters $\\eta = (\\mu_w, \\sigma_w, \\mu_b, \\sigma_b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "\n",
    "def guide(x, y=None):\n",
    "    w_loc = pyro.param(\"w_loc\", torch.tensor(0.))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", Normal(w_loc, w_scale))\n",
    "    b_loc = pyro.param(\"b_loc\", torch.tensor(0.))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    b = pyro.sample(\"b\", Normal(b_loc, b_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "In the previous class we studied the Evidence Lower Bound (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\nu &= \\text{arg}\\max_\\nu \\mathcal{L}(\\nu) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_\\nu - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "- The model function defines $p(\\mathcal{D}|\\theta) p (\\theta)$ \n",
    "- The guide function defines $q_\\nu(\\theta)$ \n",
    "\n",
    "Pyro offers several versions of the [ELBO](https://docs.pyro.ai/en/stable/inference_algos.html#module-pyro.infer.elbo)\n",
    "\n",
    "- `Trace_ELBO`: Default ELBO. Reduces variance of the gradients using \"Rao-Blackwellization\"\n",
    "- `TraceEnum_ELBO`: Performs exhaustive enumeration for discrete variables\n",
    "- `TraceMeanField_ELBO`: Assumes Mean-field structure. Reduce variance of gradients using analytical KL when possible\n",
    "\n",
    "> We will study the importance of gradient variance later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Once we have defined the model and guide we create an SVI object\n",
    "\n",
    "In this example we select the default ELBO and SGD with adaptive learning rate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) #Additional debug of model/guides\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "svi = pyro.infer.SVI(model=model_obs,  \n",
    "                     guide=guide,\n",
    "                     loss=pyro.infer.Trace_ELBO(), # Loss function\n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 0.01})) # Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method of SVI is \n",
    "\n",
    "- `svi.step(*args)`: Performs a gradient step, similar to the `backward()` plus `step()` in pytorch\n",
    "\n",
    "`step()` receives the inputs for guide and model as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(10, 2.5), dpi=80, tight_layout=True)\n",
    "lines = [ax_.plot([], [])[0] for ax_ in ax]\n",
    "param_names = [\"ELBO\", \"w_loc\", \"w_scale\", \"b_loc\", \"b_scale\"]\n",
    "param_evolution = {}\n",
    "for name in param_names:\n",
    "    param_evolution[name] = []\n",
    "    \n",
    "for ax_, name in zip(ax, param_names):\n",
    "    ax_.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data\n",
    "x = torch.tensor([-2., 2.])\n",
    "y = torch.tensor([-2., 2.])\n",
    "    \n",
    "for k in tqdm(range(3000)):\n",
    "    param_evolution[\"ELBO\"].append(svi.step(x, y))\n",
    "    for name in param_names[1:]:\n",
    "        param_evolution[name].append(pyro.param(name).item()) \n",
    "    \n",
    "    if np.mod(k, 100) == 0:\n",
    "        for i, name in enumerate(param_names):\n",
    "            lines[i].set_ydata(param_evolution[name][:k])\n",
    "        for line in lines:\n",
    "            line.set_xdata(range(k))\n",
    "        for ax_ in ax.ravel():\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ELBO and the parameters have converged\n",
    "\n",
    "\n",
    "## Posterior predictive \n",
    "\n",
    "We can evaluate the quality of the trained model by visualizing the posterior of $w$ and $b$ and the posterior predictive of $y$ given $x$\n",
    "\n",
    "\n",
    "For this we can use the utility class [`pyro.infer.Predictive`](http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.predictive.Predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = np.linspace(-5, 5, num=100).astype('float32') \n",
    "\n",
    "predictive = pyro.infer.Predictive(model_obs, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=5000,\n",
    "                                   return_sites=(\"w\", \"b\", \"y\"))\n",
    "\n",
    "samples = predictive(torch.from_numpy(line_x))\n",
    "\n",
    "# Posterior of w and b\n",
    "b_plot_vi = samples['b'].detach().numpy()[:, 0]\n",
    "w_plot_vi = samples['w'].detach().numpy()[:, 0]\n",
    "figure = corner.corner(np.stack((b_plot_vi, w_plot_vi)).T, \n",
    "                       smooth=1., labels=[\"bias\", \"weight\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-2, 2), (-1, 3)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive of y given x\n",
    "y_trace = samples[\"y\"].detach().numpy()\n",
    "med = np.median(y_trace, axis=0)\n",
    "qua = np.quantile(y_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(line_x, med)\n",
    "ax.fill_between(line_x, qua[0], qua[1], alpha=0.5);\n",
    "\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC with Pyro\n",
    "\n",
    "[On the MCMC side Pyro](https://docs.pyro.ai/en/stable/mcmc.html) offers Hamiltonian Monte-Carlo and the more recent No-U turn sampler (NUTS)\n",
    "\n",
    "For theoretical details see Barber Chapter 27 or [here](https://github.com/magister-informatica-uach/INFO337/tree/master/MCMC)\n",
    "\n",
    "Here we run MCMC as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "nuts_kernel = NUTS(model_obs, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=1, num_samples=1000, warmup_steps=1000)\n",
    "sampler.run(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_samples()\n",
    "w_plot_mcmc = samples['w'].detach().numpy()\n",
    "b_plot_mcmc = samples['b'].detach().numpy()\n",
    "\n",
    "figure = corner.corner(np.stack((b_plot_mcmc , w_plot_mcmc )).T, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-2, 2), (-1, 3)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too different from the VI solution\n",
    "\n",
    "But remember, in this case we used to actual posterior (normal) in the guide\n",
    "\n",
    "**Most of the time we won't be so lucky**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro summary\n",
    "\n",
    "- We create (deterministic) parameters with `pyro.param`\n",
    "- We create latent random variables using `pyro.sample`\n",
    "- We create observed random variables using `pyro.sample` with the `obs` keyword\n",
    "\n",
    "> The model represents our graphical model\n",
    "\n",
    "> The guide represents our assumptions on the latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "> Deep Neural Networks are non-linear function approximators which represent the state of the art in pattern recognition\n",
    "\n",
    "But they do have limitations\n",
    "\n",
    "- Very deep models require lots of data and can be hard to train\n",
    "- Selecting an architecture requires a lot of experimentation\n",
    "- [Not too robust](https://openai.com/blog/adversarial-example-research/)\n",
    "- Poor at representing uncertainty\n",
    "\n",
    "> We can leverage some of these by going Bayesian\n",
    "\n",
    "- A Bayesian neural network (BNN) places a prior distribution on its parameters \n",
    "- Training the BNN $\\equiv$ Learning the posterior distribution of the parameters given the data\n",
    "- The **uncertainty on the data and the parameters** can be propagated to estimate the **uncertainty on our predictions**\n",
    "    - Uncertainty on the data is called **aleatoric uncertainty** and it is related to irreducible noise\n",
    "    - Uncertainty on the model (parameters and structure) is called **epistemic uncertainty**\n",
    "\n",
    "> We know what we don't know\n",
    "\n",
    "We can use this \"new knowledge\" to\n",
    "- Decide when to use a more simple/complex model (complexity-control)\n",
    "- Decide when to take a critical decisions\n",
    "    - Autonomouse cars\n",
    "    - Cancer diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of history\n",
    "\n",
    "> Bayesian neural networks is an active area of research \n",
    "\n",
    "- 1980's: Bayes theorem applied to Neural Networks (John Hopfield and Naftali Tishby)\n",
    "- 1990's: Monte-Carlo and VI for bayesian neural networks was studied extensively by [David Mackay](http://www.inference.org.uk/mackay/BayesNets.html) and [Radford Neal](https://www.cs.toronto.edu/~radford/res-neural.html) (Also Bishop, Barber, Hinton, Gharamani and many others). Neal shows that Gaussian process are bayesian neural networks with infinite neurons\n",
    "\n",
    "> The models remain quite difficult to train for some time\n",
    "\n",
    "- 2010's: Deep learning arrives \n",
    "- 2011: [Alex Graves' VI for neural networks](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks)\n",
    "- Explosion of practical deep bayesian networks \n",
    "    - [Charles Blundell's Bayes by backprop](https://arxiv.org/abs/1505.05424)\n",
    "    - [Yarin Gal's many work](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)\n",
    "    - Durk Kingma, Danilo Jimenez Rezende, Shakir Mohamed, José Miguel Hernandez-Lobato\n",
    "- [Hot topic now a days](http://bayesiandeeplearning.org/)\n",
    "\n",
    "History in video by [Zoubin Gharamani](http://mlg.eng.cam.ac.uk/zoubin/) at [NIPS 2016](https://www.youtube.com/watch?v=FD8l2vPU5FY) and [interesting panel discussion](https://www.youtube.com/watch?v=HumFmLu3CJ8) on the same conference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More formally\n",
    "\n",
    "\n",
    "Assuming\n",
    "- $N$ *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}$ \n",
    "- $x$ is a $D$ dimensional vector\n",
    "- Fully-connected neural network with one hidden layer ($H$ neurons) for regression\n",
    "- $\\text{sgm}(\\cdot)$ is a non-linear activation function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\theta(x) &=   b_i + \\sum_{j=1}^H w_{ij} h_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} \\text{sgm} \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The parameter vector $\\theta$ contains all the weights and biases of the model\n",
    "\n",
    "**Prior:** We propose a prior for $\\theta$, typically\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "**Likelihood:** We propose a likelihood depending on our task, typically Gaussian for regression and Bernoulli/Categorical for binary/multiclass classification \n",
    "\n",
    "**Posterior:** We use Bayes theorem to write the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{1}{{p(\\mathcal{D})}} \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "Even though the likelihood and prior are normal **the posterior in this case is not normal** because of the nested nonlinearity (Can you show this?)\n",
    "\n",
    "In general:\n",
    "\n",
    "> We cannot obtain an analytical posterior for a bayesian neural network\n",
    "\n",
    "We resort to sampling-based (MCMC) or deterministic (VI) approximate inference (previous class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first Bayesian Neural Network using Pyro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same synthetic data from our previous class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding the bayesian neural net\n",
    "\n",
    "Neural nets in pyro are classes that inherit from `PyroModule`\n",
    "\n",
    "This is done using `PyroModule` and `PyroSample` from [`pyro.nn`](https://docs.pyro.ai/en/stable/nn.html)\n",
    "\n",
    "- `PyroSample` is used to declare a neural net parameter as a random variable \n",
    "- `PyroModule` is used to declare torch modules which accept random parameters\n",
    "\n",
    "In the following example we lift `torch.nn.Linear` using `PyroModule`, and add priors its parameters using `PyroSample`\n",
    "\n",
    "In this regression problem we assume that the output is Gaussian distributed\n",
    "\n",
    "The likelihood is declared with its corresponding plate in the `forward` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is coded we can use `pyro.poutine.trace` with pyro validation activated to make sure that the shapes are correct\n",
    "\n",
    "- Batch dimension is 15 (number of samples)\n",
    "- Event dimension is equal to the number of neurons for each layer\n",
    "\n",
    "Independent RV (likelihood) should be in the left while dependent (weights and biases) should be on the right\n",
    "\n",
    "This is controlled using plates and the `to_event()` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Uniform, Normal, MixtureOfDiagNormals\n",
    "\n",
    "class BayesianMLPRegression(PyroModule):\n",
    "    def __init__(self, n_hidden=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_scale)\n",
    "        # Hidden layer\n",
    "        self.hidden = PyroModule[torch.nn.Linear](1, n_hidden)\n",
    "        self.hidden.weight = PyroSample(prior.expand([n_hidden, 1]).to_event(2))\n",
    "        self.hidden.bias = PyroSample(prior.expand([n_hidden]).to_event(1))\n",
    "        # Output layer\n",
    "        self.output = PyroModule[torch.nn.Linear](n_hidden, 1)\n",
    "        self.output.weight = PyroSample(prior.expand([1, n_hidden]).to_event(2))\n",
    "        self.output.bias = PyroSample(prior.expand([1]).to_event(1))\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        mean = self.output(z).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", Uniform(0.0, 0.1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", Normal(mean, sigma), obs=y) #likelihood\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "\n",
    "model = BayesianMLPRegression()\n",
    "\n",
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the BNN: MCMC \n",
    "\n",
    "Even for a extremely simple NN and using the most advanced samplers MCMC can be inpractical\n",
    "\n",
    "(Don't try to wait for this to converge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "nuts_kernel = NUTS(model, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=1, num_samples=10000, warmup_steps=1000)\n",
    "sampler.run(x_torch, y_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of VI\n",
    "\n",
    "Propose an approximate (simple) posterior $q_\\nu(\\theta)$ and optimize so that it looks similar to the actual posterior\n",
    "\n",
    "We do this by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\nu) = \\mathbb{E}_{q_\\nu(\\theta)}[ \\log p(\\mathcal{D}|\\theta)] - \\text{KL}[q_\\nu(\\theta)|p(\\theta)]\n",
    "$$\n",
    "\n",
    "An we use $q_\\nu(\\theta)$ as our replacement for $p(\\theta|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, \\theta) p(\\theta| \\mathcal{D}) \\,d\\theta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the BNN: VI\n",
    "\n",
    "Once the model is specified we need to write a guide (approximate posterior)\n",
    "\n",
    "This can be done manually as before or using the automatic guides in `pyro.infer.autoguide`. Typically we would start we the simplest diagonal normal guide (assumes no correlation between the parameters of the BNN)\n",
    "\n",
    "Then we create an SVI object and call the `step` attribute of this object iteratively\n",
    "\n",
    "We can evaluate the posteriors of the parameters and the predictive posterior using `pyro.infer.Predictive`\n",
    "\n",
    "In what follows the neural network is trained for 10000 epochs and every 100 epochs the predictive posterior is plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True, dpi=80)\n",
    "    #ax[0].set_yscale('log')\n",
    "\n",
    "    def update_plot(k, epoch_loss, samples):\n",
    "        ax[0].cla()\n",
    "        ax[0].plot(range(k), epoch_loss[:k])\n",
    "        #ax[0].autoscale_view()\n",
    "        ax[1].cla()\n",
    "        ax[1].plot(x, y, 'k.');\n",
    "        med = np.median(samples, axis=[0])\n",
    "        qua = np.quantile(samples, (0.05, 0.95), axis=0)\n",
    "        ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "        ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Turn this on for additional debugging\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "# Create a guide\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-2)\n",
    "\n",
    "# Create SVI object\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2, 'clip_norm': 10.0}), # Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO()) # Loss function \n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch.squeeze(-1)) # Actual training step\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 100 == 0:\n",
    "        # Compute predictive posterior\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "        samples = predictive(x_test, None)['obs'].detach().numpy()\n",
    "        # Plot it\n",
    "        update_plot(k, epoch_loss, samples)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete we can use the guide as our replacement to the posterior\n",
    "\n",
    "The trained pararemeters of the guide are stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can use `pyro.infer.Predictive` to get samples from our bayesian neural network when evaluated on new inputs \n",
    "\n",
    "Here we sample \"100 neural networks\" and evaluate them on `x_test` \n",
    "\n",
    "This returns the sampled parameters (weights and biases) and outputs (obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "for k, v in predictive(x_test, None).items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian network for multi-class classification with Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create synthetic 2D data with 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='int') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0, 0.5, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "\n",
    "#X, y = sklearn.datasets.make_moons(200, noise=0.2)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code a Bayesian Neural Network with two hidden layers and normal prior in all activations\n",
    "\n",
    "For the likelihood we use the Categorical (Multinomial with $n=1$). The categorical distribution expects unnormalized probabilities (logits) as input, in this case the un-activated output of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Normal, Categorical\n",
    "\n",
    "class BayesianMLPClassifier(PyroModule):\n",
    "    def __init__(self, num_hidden=10, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_std)\n",
    "        self.layer1 = PyroModule[torch.nn.Linear](2, num_hidden)\n",
    "        self.layer1.weight = PyroSample(prior.expand([num_hidden, 2]).to_event(2))\n",
    "        self.layer1.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = PyroModule[torch.nn.Linear](num_hidden, num_hidden)\n",
    "        self.layer2.weight = PyroSample(prior.expand([num_hidden, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer3 = PyroModule[torch.nn.Linear](num_hidden, 3)\n",
    "        self.layer3.weight = PyroSample(prior.expand([3, num_hidden]).to_event(2))\n",
    "        self.layer3.bias = PyroSample(prior.expand([3]).to_event(1))        \n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.layer1(x))\n",
    "        h = self.activation(self.layer2(h))\n",
    "        p = self.layer3(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", Categorical(logits=p), obs=y) # Multiclass\n",
    "            #obs = pyro.sample(\"obs\", dist.Bernoulli(logits=p), obs=y) # Binary\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use an automatic diagonal normal guide (no covariance) and train using `Trace_ELBO`\n",
    "\n",
    "We plot the mean of the predictive posterior every 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, samples):\n",
    "    ax[0].cla()\n",
    "    p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "    zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    \n",
    "\n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(X.astype('float32'))\n",
    "y_train = torch.from_numpy(y)\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianMLPClassifier(num_hidden=100, prior_std=10.)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-1)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = svi.step(x_train, y_train)\n",
    "    if k % 100 == 0:\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=10)\n",
    "        samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "        update_plot(k, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 100 neural networks and plot four individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(9, 2), tight_layout=True)\n",
    "\n",
    "for k in range(4):\n",
    "    zz = samples[\"obs\"][k].reshape(xx.shape).detach().numpy()\n",
    "    ax[k].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[k].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these categorical samples we can compute statistics\n",
    "\n",
    "In the left we plot the mode (more repeated class) and in the right the entropy. \n",
    "\n",
    "The higher then entropy the more different the output of the neural networks (high uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "zz = torch.mode(samples[\"obs\"], dim=0)[0].reshape(xx.shape).detach().numpy()\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)/100.\n",
    "entropy = lambda p: -(p*(p+1e-32).log()).sum(dim=1)\n",
    "\n",
    "zz = entropy(p).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75)\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result using a non-bayesian neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):    \n",
    "    def __init__(self, num_hidden=10):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, num_hidden) \n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = torch.nn.Linear(num_hidden, 3)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.layer1(x))\n",
    "        z = self.activation(self.layer2(z))\n",
    "        return self.layer3(z)     \n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, model):\n",
    "    ax[0].cla()\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    zz = torch.nn.Softmax(dim=1)(Z).argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(num_hidden=100)\n",
    "display(model)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(x, y, phase='train'):\n",
    "    haty = model.forward(x) # Evaluate the model\n",
    "    loss = criterion(haty, y) # Calculate errors\n",
    "    if phase == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Compute derivatives\n",
    "        optimizer.step() # Update parameters \n",
    "    return loss.item()\n",
    "\n",
    "x_train = torch.from_numpy(X.astype('float32'))#.reshape(-1, 1)\n",
    "y_train = torch.from_numpy(y)#.reshape(-1, 1)\n",
    "epoch_loss = np.zeros(shape=(3000,)) \n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = train_one_epoch(x_train, y_train)\n",
    "    if k % 100 == 0: \n",
    "        update_plot(k, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the softmax output as probabilities we can also compute its entropy\n",
    "\n",
    "Is it the same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Z = torch.nn.Softmax(dim=1)(model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))))\n",
    "zz = Z.argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "zz = -(Z*(Z+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75, vmin=0., vmax=np.log(3))\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a known phenomenon in deep neural networks: **Miscalibration**\n",
    "\n",
    "The uncertainty of the predictions is very low even when far from the data\n",
    "\n",
    "> The network is being **over-confident**\n",
    "\n",
    "\"after (almost) all training samples are correctly classified, crossentropy (neg log likelihood) can be further minimized by increasing the confidence of the predictions (reducing entropy of softmax output)\"\n",
    "\n",
    "The uncertainty obtained from model averaging (bayesian) and the one derived from the softmax output should not be confused\n",
    "\n",
    "\n",
    "References:\n",
    "- [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)\n",
    "- [Being Bayesian, Even Just a Bit,Fixes Overconfidence in ReLU Networks](https://arxiv.org/pdf/2002.10118v1.pdf)\n",
    "- [Evidential Deep Learning to Quantify Classification Uncertainty](https://arxiv.org/pdf/1806.01768.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional bayesian network for MNIST with Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST handwritten digits dataset\n",
    "\n",
    "In this example we will use only the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_test = torchvision.datasets.MNIST(root='~/datasets', train=False, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple fully-connected neural network \n",
    "\n",
    "Can you extend this to a convolutional neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Normal, Categorical\n",
    "\n",
    "class CBNN(PyroModule):\n",
    "    def __init__(self, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_std)\n",
    "        \n",
    "        self.conv1 = PyroModule[torch.nn.Conv2d](1, 16, 3, 2) # input filters, output filters, kernel size, stride\n",
    "        self.conv1.weight = PyroSample(prior.expand([16, 1, 3, 3]).to_event(4))\n",
    "        self.conv1.bias = PyroSample(prior.expand([16]).to_event(1))\n",
    "        \n",
    "        self.conv2 = PyroModule[torch.nn.Conv2d](16, 16, 3, 2)\n",
    "        self.conv2.weight = PyroSample(prior.expand([16, 16, 3, 3]).to_event(4))\n",
    "        self.conv2.bias = PyroSample(prior.expand([16]).to_event(1))\n",
    "        \n",
    "        self.fc1 = PyroModule[torch.nn.Linear](16*6*6, 100)\n",
    "        self.fc1.weight = PyroSample(prior.expand([100, 16*6*6]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(prior.expand([100]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroModule[torch.nn.Linear](100, 10)\n",
    "        self.fc2.weight = PyroSample(prior.expand([10, 100]).to_event(2))\n",
    "        self.fc2.bias = PyroSample(prior.expand([10]).to_event(1))\n",
    "        \n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.conv1(x))\n",
    "        h = self.activation(self.conv2(h))\n",
    "        h = h.reshape(-1, 16*6*6)\n",
    "        h = self.activation(self.fc1(h))\n",
    "        p = self.fc2(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", Categorical(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with diagonal normal guide and `Trace_ELBO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7, 3), tight_layout=True)\n",
    "line = ax.plot([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = CBNN(prior_std=10.)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-1)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, shuffle=True)\n",
    "epoch_loss = np.zeros(shape=(100,))\n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    for images, labels in mnist_loader:\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss[k] += svi.step(images, labels)\n",
    "    #break    \n",
    "    if k % 1 == 0:\n",
    "        line[0].set_xdata(range(k))\n",
    "        line[0].set_ydata(epoch_loss[:k])\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mode and entropy from the predictive samples\n",
    "\n",
    "With this we can explore the digits for which the model is most uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(mnist_test.data.unsqueeze(1)/255.)\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=10).sum(dim=0)/100.\n",
    "mode = p.argmax(dim=1)\n",
    "entropy = -(p*(p+1e-32).log()).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=mnist_test.targets.detach().numpy(),\n",
    "                            y_pred=mode.detach().numpy(),\n",
    "                            digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 2.5), tight_layout=True)\n",
    "\n",
    "digit = 4\n",
    "mask = mnist_test.targets == digit\n",
    "idx = np.argsort(entropy[mask].numpy())[::-1]\n",
    "k = 0\n",
    "def update(x):\n",
    "    global k\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    ax[0].imshow(mnist_test.data[mask][idx[k]], cmap=plt.cm.Greys_r)\n",
    "    res = ax[1].hist(samples['obs'][:, mask][:, idx[k]], range=(0, 10))\n",
    "    ax[1].set_title(\"%d %0.4f\" %(mode[mask][idx[k]], entropy[mask][idx[k]]))\n",
    "    ax[1].set_xticks(range(10));\n",
    "    k+=1\n",
    "\n",
    "bnext = widgets.Button(description='next')\n",
    "bnext.on_click(update)\n",
    "bnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Bayesian Neural Networks Training\n",
    "\n",
    "- State of the art!\n",
    "- Very delicate: bad initializations and local minima \n",
    "- Set appropriate priors\n",
    "- Variance control and reparameterization (more on this next class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
