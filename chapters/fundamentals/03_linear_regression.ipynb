{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import corner \n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "## Summary of OLS\n",
    "\n",
    "In linear regression we have a \n",
    "\n",
    "- continuous D-dimensional input $x$ \n",
    "- continuous one-dimensional target $y$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "b + \\sum_{d=1}^D w_d x_d = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_D)$\n",
    "\n",
    "Typically, we fit this model by minimizing the squared errors\n",
    "\n",
    "$$\n",
    "\\min_\\theta\\sum_n \\left(y_n - f_\\theta(x_n) \\right)^2 = (Y - \\Phi \\theta)^T (Y - \\Phi \\theta)\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix},~  Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} ~\\text{and}~  \\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "Linear regression is linear on $\\theta$. If we apply non-linear transformations on $x$ we obtain the same solution for $\\theta$. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_d w_d x^d + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_d \\alpha_d \\cos(2\\pi d f_0 x)  + \\sum_d \\beta_d \\sin(2\\pi d f_0 x) + c$ \n",
    "\n",
    "In what follows we will review the probabilistic interpretation of OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f_\\theta(x) + \\epsilon \\nonumber \\\\\n",
    "&= b + \\sum_{d=1}^D w_d x_d   + \\epsilon, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed (iid) with variance $\\sigma_\\epsilon^2$ then\n",
    "\n",
    "$$\n",
    "p(y|x, \\theta) = \\mathcal{N}\\left(y| f_\\theta(x) , \\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta~$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "The prior on the parameters gives us the space of possible models (before presenting data)\n",
    "\n",
    "We can evaluate the models arising from these parameter combinations on the test data \n",
    "\n",
    "These correspond to samples from the **prior predictive distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-5, 5, num=100)[:, None].astype('float32') #100x1\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    torch.nn.init.normal_(linear_layer.weight, 0.0, sw)\n",
    "    torch.nn.init.normal_(linear_layer.bias, 0.0, sb)\n",
    "    #y = W*x + b\n",
    "    y_test = linear_layer(torch.from_numpy(x_test)).detach().numpy()\n",
    "    ax.plot(x_test, y_test, c='tab:blue', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of feasible solutions is constrained by by presenting data\n",
    "\n",
    "### Point-estimate solution (MAP)\n",
    "\n",
    "For a dataset $\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\}$\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta~$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ \\mathcal{N} (\\theta|0, \\Sigma_\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2} \\theta^T \\Sigma_\\theta^{-1} \\theta  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the log likelihood is\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) = \\sum_{n=1}^N \\log \\mathcal{N}(y_n|f_\\theta(x_n),\\sigma_\\epsilon^2)\n",
    "$$\n",
    "\n",
    "and the result is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda )^{-1} \\Phi^T Y\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\sigma_\\epsilon^2 \\Sigma_\\theta^{-1}$\n",
    "\n",
    "> This is the ridge regression or **regularized least squares** solution\n",
    "\n",
    "What happens if the variance of the prior tends to infinity? This would be an uninformative prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the parameters\n",
    "\n",
    "In this case we want the posterior of $\\theta~$ given the dataset\n",
    "\n",
    "Assuming that we know $\\sigma_\\epsilon$ we can write \n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto  \\mathcal{N}(Y| \\Phi \\theta, I\\sigma_\\epsilon^2) \\mathcal{N}(\\theta| \\theta_0, \\Sigma_{\\theta_0})\n",
    "$$\n",
    "\n",
    "The likelihood is gaussian and the prior is gaussian, so\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto \\frac{1}{Z} \\exp \\left ( -\\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta)  - \\frac{1}{2} (\\theta - \\theta_{0})^{T} \\Sigma_{\\theta_0}^{-1} (\\theta - \\theta_0)\\right)\n",
    "$$\n",
    "\n",
    "and (with a bit of algebra) it can be shown that this corresponds to a gaussian distribution (gaussian is conjugate to itself)\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) = \\mathcal{N}(\\theta|\\theta_1, \\Sigma_{\\theta_1} )\n",
    "$$\n",
    "\n",
    "with parameters \n",
    "$$\n",
    "\\Sigma_{\\theta_1} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2  \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\Sigma_{\\theta_1} \\Sigma_{\\theta_0}^{-1} \\theta_{0} + \\frac{1}{\\sigma_\\epsilon^2} \\Sigma_{\\theta_1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "> **Iterative framework:** We can present data and update the distribution of $\\theta~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Fitting a line**\n",
    "\n",
    "We assume a zero-mean and diagonal covariance gaussian prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "mw, mb = 0., 0.\n",
    "sw, sb = 5., 5.\n",
    "So = np.diag(np.array([sb, sw])**2)\n",
    "mo = np.array([mb, mw])\n",
    "seps = 1. # What happens if this is larger/smaller?\n",
    "\n",
    "theta_plot = np.random.multivariate_normal(mo, So, size=10000)\n",
    "\n",
    "# A look at the empirical prior distribution\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe data at $x=2$, $y=2$ and compute the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "Phi = np.array([[1.0, 2.0]])\n",
    "y = np.array([2.0])\n",
    "\n",
    "# The updated parameters\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)\n",
    "\n",
    "# The empirical posteriorthe\n",
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this the space of possible models is constrained, as shown by the samples of the **posterior predictive distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y_test = linear_layer(torch.from_numpy(x_test)).detach().numpy()\n",
    "    ax.plot(x_test, y_test, c='tab:blue', alpha=0.25)\n",
    "\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we observe a additional data point at $x=-2$, $y=-2$\n",
    "\n",
    "We will use the previous posterior as prior and update the parameters again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "So = Sn\n",
    "mo = mn\n",
    "#Update\n",
    "Phi = np.array([[1.0, -2.0]])\n",
    "y = np.array([-2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)\n",
    "\n",
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of possible models is further reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y_test = linear_layer(torch.from_numpy(x_test)).detach().numpy()\n",
    "    ax.plot(x_test, y_test, c='tab:blue', alpha=0.2)\n",
    "\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the predictions\n",
    "\n",
    "Don't forget our goal\n",
    "\n",
    "> We want a model to predict $y$ for unseen examples $x$ given our training set\n",
    "\n",
    "We can write this in \"bayesian\" as $p(y | x, \\mathcal{D})$, the **posterior predictive distribution**\n",
    "\n",
    "Previously we looked at samples from this distribution. We can obtain an expression for it by marginalizing $\\theta$ in\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y | x, \\mathcal{D}) &= \\int p(y, \\theta | x, \\mathcal{D}) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x, \\mathcal{D}) p(\\theta| \\mathcal{D}, x) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x) p(\\theta| \\mathcal{D}) d\\theta, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "note that $y$ is conditionally independant of $\\mathcal{D}$ given $\\theta$\n",
    "\n",
    "For our linear regression\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|x, \\mathcal{D}, \\sigma_\\epsilon^2) &= \\int p(y|f_\\theta(x), \\sigma_\\epsilon^2) p(\\theta| \\theta_{N}, \\Sigma_{\\theta_N}) d\\theta \\nonumber \\\\\n",
    "&= \\mathcal{N}\\left(y|f_{\\theta_N} (x), \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**The posterior predictive is Gaussian**, because the convolution of gaussians is gaussian\n",
    "\n",
    "If we consider that $N$ samples were presented and that $\\mu_0=0$ then \n",
    "\n",
    "$$\n",
    "\\theta_{N} =  (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "which is the MAP estimator, and\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta_N} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the variance (uncertainty) for the new $x$ is \n",
    "$$\n",
    "\\sigma^2(x) = \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\n",
    "$$\n",
    "\n",
    "> The variance of the prediction has contribution from the noise (irreducible) and the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty grows when we depart from the observed data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Phi_x = np.vstack(([1]*100, x_test[:, 0])).T\n",
    "sx = np.sqrt(np.diag(seps**2 + np.dot(np.dot(Phi_x, Sn), Phi_x.T)))\n",
    "ax.plot(x_test, np.dot(Phi_x, mn), '--')\n",
    "ax.fill_between(x_test[:, 0], np.dot(Phi_x, mn)-2*sx, np.dot(Phi_x, mn)+2*sx, alpha=0.5)\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:**\n",
    "\n",
    "See how the posterior predictive distribution changes with increasing/decreasing $\\sigma_\\epsilon$ and $\\Sigma_{\\theta_0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study\n",
    "\n",
    "- [Chapter 18.1 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "- In all this we assumed $\\sigma_\\epsilon$ known. For a bayesian treatment with unknown noise variance we would use a normal inverse gamma prior (this is covered in Barber 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
