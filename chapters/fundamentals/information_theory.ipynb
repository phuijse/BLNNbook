{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory and Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we measure information?\n",
    "\n",
    "> What is information? Can we measure it?\n",
    "\n",
    "Information Theory is the mathematical study of the quantification and transmission of information proposed by **Claude Shannon** on this seminal work: *A Mathematical Theory of Communication*, 1948\n",
    "\n",
    "Shannon considered the output of a noisy source as a random variable $X$ taking $M$ possible values $\\mathcal{A} = \\{x_1, x_2, x_3, \\ldots, x_M\\}$\n",
    "\n",
    "Each value $x_i$ have an associated probability $P(X=x_i) = p_i$\n",
    "\n",
    "> What is the amount of information carried by $x_i$?\n",
    "\n",
    "Shannon defined the amount of information as\n",
    "\n",
    "$$\n",
    "I(x_i) = \\log_2 \\frac{1}{p_i},\n",
    "$$\n",
    "\n",
    "which is measured in **bits**\n",
    "\n",
    "> One bit is the amount of information needed to choose between two **equiprobable** states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** A meteorological station that sends tomorrow's weather prediction\n",
    "\n",
    "The dictionary of messages: (1) Rainy, (2) Cloudy, (3) Partially cloudy, (4) Sunny\n",
    "\n",
    "Their probabilities are: $p_1=1/2$, $p_2=1/4$, $p_3=1/8$, $p_4=1/8$\n",
    "\n",
    "The minimum number of yes/no questions (equiprobable) needed to guess tomorrow's weather:\n",
    "\n",
    "- Is it going to rain? \n",
    "- No: Is it going to be cloudy?\n",
    "- No: Is it going to be sunny?\n",
    "\n",
    "Amount of information:\n",
    "\n",
    "- Rainy: $\\log_2 \\frac{1}{p_1} = \\log_2 2 = 1$ bits\n",
    "- Cloudy: $2$ bits \n",
    "- Partially cloudy and Sunny: $3$ bits\n",
    "\n",
    "> The larger the probability the smallest information it carries\n",
    "\n",
    "> Amount of information is also called surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon's entropy\n",
    "\n",
    "After defining the amount of information for a state Shannon's defined the average information of the source $X$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= \\mathbb{E}_{x\\sim X}\\left [\\log_2 \\frac{1}{P(x)} \\right] \\nonumber \\\\\n",
    "&= - \\sum_{x\\in \\mathcal{A}} P(x) \\log_2 P(X)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^M p_i \\log_2 p_i  ~ \\text{[bits]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and called it the **entropy** of the source\n",
    "\n",
    "> Entropy is the \"average information of the source\"\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "- Entropy is nonnegative: $H(X)>0$\n",
    "- Entropy is equal to zero when $p_j = 1 \\wedge p_i = 0, i \\neq j$\n",
    "- Entropy is maximum when $X$ is uniformly distributed $p_i = \\frac{1}{M}$, $H(X) = \\log_2(M)$\n",
    "\n",
    "> The more random the source is the larger its entropy\n",
    "\n",
    "Differential entropy for continuous variables as \n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) \\,dx ~ \\text{[nats]}\n",
    "$$\n",
    "\n",
    "where $p(x)$ is the probability density function (pdf) of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy: Kullback-Leibler (KL) divergence\n",
    "\n",
    "Consider a continuous random variable $X$ and two distributions $q(x)$ and $p(x)$ defined on its probability space\n",
    "\n",
    "The relative entropy between these distributions is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] &= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log \\frac{p(x)}{q(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log p(x) \\right ]  - \\mathbb{E}_{x \\sim p(x)} \\left [ \\log q(x) \\right ],  \\nonumber \\\\\n",
    "&= \\int p(x) \\log p(x) \\,dx  - \\int p(x) \\log q(x) \\,dx  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is also known as the **[Kullback](https://en.wikipedia.org/wiki/Solomon_Kullback)- [Leibler](https://en.wikipedia.org/wiki/Richard_Leibler) divergence**\n",
    "\n",
    "- The left hand side term is the negative entropy of $p(x)$\n",
    "- The right hand side term is called the **cross-entropy of $q(x)$ relative to $p(x)$** \n",
    "\n",
    "**Intepretations of KL**\n",
    "\n",
    "- Coding: Expected number of \"extra bits\" needed to code $p(x)$ using a code optimal for $q(x)$\n",
    "- Bayesian modeling: Amount of information lost when $q(x)$ is used as a model for $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property**: Non-negativity\n",
    "\n",
    "The KL divergence is non-negative\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\geq 0\n",
    "$$\n",
    "with the equality holding for $p(x) \\equiv q(x)$\n",
    "\n",
    "This is given by the [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "\n",
    "$$\n",
    "- \\int p(x) \\log p(x) \\,dx  \\leq - \\int p(x) \\log q(x) \\,dx \n",
    "$$\n",
    "\n",
    "> then entropy of $p(x)$ is equal or less than the cross-entropy of $q(x)$ relative to $p(x)$\n",
    "\n",
    "\n",
    "**Property**: Asymmetry\n",
    "\n",
    "The KL divergence is asymmetric\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\neq D_{\\text{KL}} \\left [ q(x) || p(x) \\right]\n",
    "$$\n",
    "\n",
    "- The KL is not a proper distance (no triangle inequility either)\n",
    "- Forward and Reverse KL have different meanings (we will explore them soon)\n",
    "\n",
    "\n",
    "**Property**: Relation with mutual information\n",
    "\n",
    "The KL is related to the mutual information between random variables as\n",
    "\n",
    "$$\n",
    "\\text{MI}(X, Y) = D_{\\text{KL}} \\left [ p(x, y) || p(x)p(y) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic generative models\n",
    "\n",
    "Let's say we have $N$ continuous observations \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{x_1, x_2, \\ldots, x_N\\}\n",
    "$$ \n",
    "\n",
    "(Assuming that they are iid) there is unknown distribution that generated these samples\n",
    "\n",
    "$$\n",
    "x_i \\sim p^*(x)\n",
    "$$\n",
    "\n",
    "The goal of **generative modeling** is to learn a probabilistic model \n",
    "\n",
    "$$\n",
    "p_\\theta(x)\n",
    "$$ \n",
    "\n",
    "with parameters $\\theta~$ that \"mimics\" $p^*(x)$\n",
    "\n",
    "In a few words:\n",
    "\n",
    "> match  $p_\\theta (x)$ to $p^*(x)$\n",
    "\n",
    "After matching $p_\\theta(x)$ we can use it to sample new data: **generation**\n",
    "\n",
    "Later we will extend this definition to **joint distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recipe to fit generative models\n",
    "\n",
    "1. Select a parametric form for $p_\\theta (x)$\n",
    "1. Write the difference between $p_\\theta (x)$  and $p^*(x)$\n",
    "1. Minimize this **difference** as a function of $\\theta~$\n",
    "\n",
    "> How do we compute the difference between probability distributions?\n",
    "\n",
    "We could use the **forward** KL divergence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\min_\\theta D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right]  \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\theta \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]  - \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Bad news:** We can't evaluate $\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]$ \n",
    "\n",
    "**Good news:** That term doesn't depend on $\\theta~$, if we only care about optimization we can drop it\n",
    "\n",
    "$$\n",
    "\\min_\\theta D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] = \\max_\\theta\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ]\n",
    "$$\n",
    "\n",
    "Now if we approximate the expected value with an average over our finite dataset\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\approx \\sum_{i=1}^N \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "What we obtain is the log likelihood of $\\theta~$!\n",
    "\n",
    "> Minimizing the forward KL divegence $\\equiv$ Maximizing the log likelihood of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Forward KL fitting of a univariate Gaussian model\n",
    "\n",
    "- We have a sample $\\{x_i\\}$ with $i=1,\\ldots,N$\n",
    "- We propose a model $p_\\theta(x) = \\mathcal{N}(x|\\mu, \\sigma^2)$ with parameters $\\theta=(\\mu, \\sigma)$\n",
    "- We write the log likelihood\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\approx  \\sum_{i=1}^N \\log p_\\theta(x_i) =  -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Let's do forward KL with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normal_model():\n",
    "    def __init__(self):\n",
    "        self.mu = torch.tensor([0.], requires_grad=True)\n",
    "        self.s2 = torch.tensor([10.], requires_grad=True)\n",
    "        self.optimizer = torch.optim.Adam([self.mu, self.s2], lr=1e-1)\n",
    "        \n",
    "    def neg_log_likelihood(self, data):\n",
    "        N = data.shape[0]\n",
    "        return 0.5*(data - self.mu).pow(2).sum()/self.s2 + 0.5*N*torch.log(2*np.pi*self.s2)\n",
    "    \n",
    "    def update(self, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.neg_log_likelihood(data)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.mu.detach().numpy(), self.s2.detach().numpy()\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        return torch.exp(-0.5*(x - self.mu).pow(2)/self.s2)/torch.sqrt(2.*np.pi*self.s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create gaussian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "data = scipy.stats.norm(loc=5., scale=2.).rvs(1000) # N(5, sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how fitting the model step by step using SGD looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75, label='data');\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "data_torch = torch.from_numpy(data.astype('float32'))\n",
    "model = normal_model()\n",
    "line = ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2, label='model')\n",
    "ax.legend()\n",
    "\n",
    "def update_plot(k):\n",
    "    model.update(data_torch)\n",
    "    line[0].set_ydata(model.pdf(x_plot).detach().numpy())\n",
    "    ax.set_title(\"mu=%0.3f s2=%0.3f\" % model.get_params())\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=100, interval=20, \n",
    "                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the model is misspecified?\n",
    "\n",
    "Let's create data from a mixture of gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = scipy.stats.bernoulli(0.7).rvs(1000)\n",
    "G1 = scipy.stats.norm(loc=5., scale=2.).rvs(1000) # N(5, sqrt(2))\n",
    "G2 = scipy.stats.norm(loc=-2., scale=1.5).rvs(1000) # N(0, sqrt(10))\n",
    "data = np.concatenate((G1[p==1], G2[p==0])) # Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75, label='data');\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "data_torch = torch.from_numpy(data.astype('float32'))\n",
    "model = normal_model()\n",
    "line = ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2, label='model')\n",
    "plt.legend()\n",
    "\n",
    "def update_plot(k):\n",
    "    model.update(data_torch)\n",
    "    line[0].set_ydata(model.pdf(x_plot).detach().numpy())\n",
    "    ax.set_title(\"mu=%0.3f s2=%0.3f\" % model.get_params())\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=100, interval=20, \n",
    "                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $p_\\theta$ spreads out to cover all the mass of the $p^*$\n",
    "\n",
    "If we go back to the definition of the KL divergence\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] = \\int p^*(x) \\log \\frac{p^*(x)}{p_\\theta(x)} \\,dx\n",
    "$$\n",
    "\n",
    "we have that by construction \n",
    "\n",
    "- For $x$ where $p^* = 0$ I don't care what $p_\\theta$ does\n",
    "- For $x$ where $p^* > 0$, if $p_\\theta \\to 0$ then $D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] \\to \\infty$\n",
    "\n",
    "> Forward KL will put $p_\\theta$ mass in all places where $p^* > 0$\n",
    "\n",
    "Because of this, forward KL is often referred to as \"mean seeking\" and \"zero-avoiding\"\n",
    "\n",
    "> It favors a diverse but not so realistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Reverse KL fitting of MoG data using univariate Gaussian model\n",
    "\n",
    "\n",
    "What happens in the misspecified case if we use reverse KL instead?\n",
    "\n",
    "The reverse KL is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}}\\left [ p_\\theta(x) || p^*(x) \\right] &= \\int p_\\theta(x) \\log \\frac{p_\\theta(x)}{p^*(x)} \\,dx \\nonumber \\\\\n",
    "&= \\int p_\\theta(x) \\log p_\\theta(x) \\,dx - \\int p_\\theta(x) \\log p^*(x) \\,dx \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Can you recognize the first term?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normal_model():\n",
    "    def __init__(self):\n",
    "        self.mu = torch.tensor(0., requires_grad=True)\n",
    "        self.s2 = torch.tensor(10., requires_grad=True)\n",
    "        self.optimizer = torch.optim.Adam([self.mu, self.s2], lr=1e-1)\n",
    "        \n",
    "    def reverse_kl(self, data):\n",
    "        model_approx = torch.distributions.normal.Normal(0.0, 1.0)\n",
    "        samples = self.mu + torch.sqrt(self.s2)*model_approx.sample(torch.Size([100, 1]))\n",
    "        entropy = -0.5*torch.log(2*np.pi*self.s2) - 0.5\n",
    "        return  entropy - 2.5*torch.logsumexp(-0.5*(data.T - samples).pow(2)/0.1**2, dim=[0, 1])\n",
    "    \n",
    "    def update(self, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.reverse_kl(data)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.mu.detach().numpy(), self.s2.detach().numpy()\n",
    "    \n",
    "    def pdf(self, x):\n",
    "        return torch.exp(-0.5*(x - self.mu).pow(2)/self.s2)/torch.sqrt(2.*np.pi*self.s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75, label='data');\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "data_torch = torch.from_numpy(data.astype('float32'))\n",
    "model = normal_model()\n",
    "line = ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2, label='model')\n",
    "ax.legend()\n",
    "        \n",
    "def update_plot(k):\n",
    "    model.update(data_torch)\n",
    "    line[0].set_ydata(model.pdf(x_plot).detach().numpy())\n",
    "    ax.set_title(\"mu=%0.3f s2=%0.3f\" % model.get_params())\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=100, interval=20, \n",
    "                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we need to get small (minimum) reverse KL?\n",
    "\n",
    "- For $x$ where $p_\\theta = 0$ we don't need to fit $p^*$ at all\n",
    "- For $x$ where $p_\\theta > 0$ we need to be as close to $p^*$ as possible\n",
    "- For $x$ where $p^* = 0$ we need to have $p_\\theta = 0$\n",
    "\n",
    "Also minimizing the reverse KL requires maximizing the entropy of $p_\\theta$, i.e. its mass has to be as spread as possible \n",
    "\n",
    "> In Reverse KL some portions of $p^*$ will not get mass from $p_\\theta$. The mass does not collapse to a single point thanks to max-entropy condition\n",
    "\n",
    "Reverse KL is referred as \"mode seeking\" and \"zero-forcing\"\n",
    "\n",
    "> Favors a realistic but no so diverse model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The reverse KL example is hacked!**\n",
    "\n",
    "Because I'm using an estimation of $p^*$ which I don't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75, label='data');\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "ax.plot(x_plot, scipy.stats.gaussian_kde(data).pdf(x_plot), lw=2, label='KDE')\n",
    "ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2, label='model')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we don't have $p^*$ so we can't evaluate $x\\sim p_\\theta(x)$ on it\n",
    "\n",
    "Variational Inference overcomes this by working on lower bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study and recommended reading\n",
    "\n",
    "- [Chapter 28 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "- Daniel Commenges, [\"Information Theory and Statistics: an overview\"](https://arxiv.org/pdf/1511.00860.pdf)\n",
    "- Colin Raffel, [\"GAN and Divergence Minimization\"](https://colinraffel.com/blog/gans-and-divergence-minimization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
