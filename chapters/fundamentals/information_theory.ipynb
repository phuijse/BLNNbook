{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theoretic Quantities\n",
    "\n",
    "The following presents key concepts of Information Theory that will be used later to train generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we measure information?\n",
    "\n",
    "Information Theory is the mathematical study of the quantification and transmission of information proposed by **Claude Shannon** on this seminal work: *A Mathematical Theory of Communication*, 1948\n",
    "\n",
    "Shannon considered the output of a noisy source as a random variable $X$ \n",
    "\n",
    "- The RV takes $M$ possible values $\\mathcal{A} = \\{x_1, x_2, x_3, \\ldots, x_M\\}$\n",
    "- Each value $x_i$ have an associated probability $P(X=x_i) = p_i$\n",
    "\n",
    "Consider the following question: What is the amount of information carried by $x_i$?\n",
    "\n",
    "Shannon defined the amount of information as\n",
    "\n",
    "$$\n",
    "I(x_i) = \\log_2 \\frac{1}{p_i},\n",
    "$$\n",
    "\n",
    "which is measured in **bits**\n",
    "\n",
    ":::{note}\n",
    "\n",
    "One bit is the amount of information needed to choose between two **equiprobable** states\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** A meteorological station in 1920 that sends tomorrow's weather prediction from Niebla to [Valdivia](https://en.wikipedia.org/wiki/Valdivia) via telegraph\n",
    "\n",
    "<img src=\"images/telegraph.png\" width=\"300\">\n",
    "\n",
    "Tomorrows weather is a random variable\n",
    "\n",
    "- The dictionary of messages: (1) Rainy, (2) Cloudy, (3) Partially cloudy, (4) Sunny\n",
    "- Assume thta their probabilities are: $p_1=1/2$, $p_2=1/4$, $p_3=1/8$, $p_4=1/8$\n",
    "\n",
    "What is the minimum number of yes/no questions (equiprobable) needed to guess tomorrow's weather?\n",
    "\n",
    "- Is it going to rain? \n",
    "- No: Is it going to be cloudy?\n",
    "- No: Is it going to be sunny?\n",
    "\n",
    "What is then the amount amount of information of each message?\n",
    "\n",
    "- Rainy: $\\log_2 \\frac{1}{p_1} = \\log_2 2 = 1$ bits\n",
    "- Cloudy: $2$ bits \n",
    "- Partially cloudy and Sunny: $3$ bits\n",
    "\n",
    ":::{important}\n",
    "\n",
    "The larger the probability the smallest information it carries\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's entropy\n",
    "\n",
    "After defining the amount of information for a state Shannon's defined the average information of the source $X$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= \\mathbb{E}_{x\\sim X}\\left [\\log_2 \\frac{1}{P(x)} \\right] \\nonumber \\\\\n",
    "&= - \\sum_{x\\in \\mathcal{A}} P(x) \\log_2 P(X)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^M p_i \\log_2 p_i  ~ \\text{[bits]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and called it the **entropy** of the source\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Entropy is the \"average information of the source\"\n",
    "\n",
    ":::\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "- Entropy is nonnegative: $H(X)>0$\n",
    "- Entropy is equal to zero when $p_j = 1 \\wedge p_i = 0, i \\neq j$\n",
    "- Entropy is maximum when $X$ is uniformly distributed $p_i = \\frac{1}{M}$, $H(X) = \\log_2(M)$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "The more random the source is the larger its entropy\n",
    "\n",
    ":::\n",
    "\n",
    "For continuous variables the Differential entropy is defined as \n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) \\,dx ~ \\text{[nats]}\n",
    "$$\n",
    "\n",
    "where $p(x)$ is the probability density function (pdf) of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy: Kullback-Leibler (KL) divergence\n",
    "\n",
    "Consider a continuous random variable $X$ and two distributions $q(x)$ and $p(x)$ defined on its probability space\n",
    "\n",
    "The relative entropy between these distributions is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] &= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log \\frac{p(x)}{q(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log p(x) \\right ]  - \\mathbb{E}_{x \\sim p(x)} \\left [ \\log q(x) \\right ],  \\nonumber \\\\\n",
    "&= \\int p(x) \\log p(x) \\,dx  - \\int p(x) \\log q(x) \\,dx  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is also known as the **[Kullback](https://en.wikipedia.org/wiki/Solomon_Kullback)- [Leibler](https://en.wikipedia.org/wiki/Richard_Leibler) divergence**\n",
    "\n",
    "- The left hand side term is the negative entropy of $p(x)$\n",
    "- The right hand side term is called the **cross-entropy of $q(x)$ relative to $p(x)$** \n",
    "\n",
    "**Intepretations of KL**\n",
    "\n",
    "- Coding: Expected number of \"extra bits\" needed to code $p(x)$ using a code optimal for $q(x)$\n",
    "- Bayesian modeling: Amount of information lost when $q(x)$ is used as a model for $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property**: Non-negativity\n",
    "\n",
    "The KL divergence is non-negative\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\geq 0\n",
    "$$\n",
    "\n",
    "with the equality holding for $p(x) \\equiv q(x)$\n",
    "\n",
    "This is given by the [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "\n",
    "$$\n",
    "- \\int p(x) \\log p(x) \\,dx  \\leq - \\int p(x) \\log q(x) \\,dx \n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "The entropy of $p(x)$ is equal or less than the cross-entropy of $q(x)$ relative to $p(x)$\n",
    "\n",
    ":::\n",
    "\n",
    "**Property**: Asymmetry\n",
    "\n",
    "The KL divergence is asymmetric\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\neq D_{\\text{KL}} \\left [ q(x) || p(x) \\right]\n",
    "$$\n",
    "\n",
    "- The KL is not a proper distance (no triangle inequility either)\n",
    "- Forward and Reverse KL have different meanings (we will explore them soon)\n",
    "\n",
    "\n",
    "**Property**: Relation with mutual information\n",
    "\n",
    "The KL is related to the mutual information between random variables as\n",
    "\n",
    "$$\n",
    "\\text{MI}(X, Y) = D_{\\text{KL}} \\left [ p(x, y) || p(x)p(y) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "See D. Mackays' book chapter 1 for more details on Information Theory\n",
    "\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
