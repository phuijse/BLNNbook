{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[0., 0.]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "This distribution does not support D = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5449c9530364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m prior = GaussianScaleMixture(component_logits=torch.tensor([0.5, 0.5]), coord_scale=torch.tensor([1.]), \n\u001b[0;32m----> 2\u001b[0;31m                              component_scale=torch.tensor([1., 0.01]))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pyro-env/lib/python3.7/site-packages/pyro/distributions/gaussian_scale_mixture.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, coord_scale, component_logits, component_scale)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This distribution does not support D = 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The coord_scale parameter in GaussianScaleMixture should be D dimensional\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: This distribution does not support D = 1"
     ]
    }
   ],
   "source": [
    "prior = GaussianScaleMixture(component_logits=torch.tensor([0.5, 0.5]), coord_scale=torch.tensor([1.]), \n",
    "                             component_scale=torch.tensor([1., 0.01]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand([10]).to_event(2).shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by([10]).to_event(1).rsample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroSample(prior.expand_by((10, 1)).to_event(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixtureOfDiagNormals??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Uniform, Normal, GaussianScaleMixture\n",
    "\n",
    "class BayesianMLPRegression(PyroModule):\n",
    "    def __init__(self, n_hidden=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "        #prior = Normal(0, prior_scale)\n",
    "        prior = MixtureOfDiagNormals(locs=torch.tensor([[0.], [0.]]),\n",
    "                                     coord_scale=torch.tensor([[prior_scale], [0.01]]),\n",
    "                                     component_logits=torch.tensor([0.5, 0.5]))\n",
    "        # Hidden layer\n",
    "        #display(prior.expand([n_hidden, 1]).to_event(2).shape())\n",
    "        self.hidden = PyroModule[torch.nn.Linear](1, n_hidden)\n",
    "        self.hidden.weight = PyroSample(prior.expand_by([n_hidden, 1]).to_event(2))\n",
    "        self.hidden.bias = PyroSample(prior.expand_by([n_hidden]).to_event(1))\n",
    "        # Output layer\n",
    "        self.output = PyroModule[torch.nn.Linear](n_hidden, 1)\n",
    "        self.output.weight = PyroSample(prior.expand([1, n_hidden]).to_event(2))\n",
    "        self.output.bias = PyroSample(prior.expand([1]).to_event(1))\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        mean = self.output(z).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", Uniform(0.0, 0.1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", Normal(mean, sigma), obs=y) #likelihood\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "\n",
    "model = BayesianMLPRegression()\n",
    "\n",
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True, dpi=80)\n",
    "#ax[0].set_yscale('log')\n",
    "\n",
    "def update_plot(k, epoch_loss, samples):\n",
    "    ax[0].cla()\n",
    "    ax[0].plot(range(k), epoch_loss[:k])\n",
    "    #ax[0].autoscale_view()\n",
    "    ax[1].cla()\n",
    "    ax[1].plot(x, y, 'k.');\n",
    "    med = np.median(samples, axis=[0])\n",
    "    qua = np.quantile(samples, (0.05, 0.95), axis=0)\n",
    "    ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "    ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Turn this on for additional debugging\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "# Create a guide\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-2)\n",
    "\n",
    "# Create SVI object\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2, 'clip_norm': 10.0}), # Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO()) # Loss function \n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch.squeeze(-1)) # Actual training step\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 100 == 0:\n",
    "        # Compute predictive posterior\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "        samples = predictive(x_test, None)['obs'].detach().numpy()\n",
    "        # Plot it\n",
    "        update_plot(k, epoch_loss, samples)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "#x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "#x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "#y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y, n_hidden=10):\n",
    "\n",
    "    \n",
    "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((1, n_hidden)), \n",
    "                                          jnp.ones((1, n_hidden))))  # D_X D_H\n",
    "    \n",
    "    b1 = numpyro.sample(\"b1\", dist.Normal(jnp.zeros((n_hidden,)), \n",
    "                                          jnp.ones((n_hidden,))))  # D_X D_H\n",
    "    \n",
    "    z1 = jnp.tanh(jnp.matmul(x, w1) + b1)   \n",
    "    \n",
    "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((n_hidden, n_hidden)), \n",
    "                                          jnp.ones((n_hidden, n_hidden))))  # D_H D_H\n",
    "    b2 = numpyro.sample(\"b2\", dist.Normal(jnp.zeros((n_hidden,)), \n",
    "                                          jnp.ones((n_hidden,))))  # D_X D_H\n",
    "    z2 = jnp.tanh(jnp.matmul(z1, w2) + b2)  # N D_H  <= second layer of activations\n",
    "\n",
    "    # sample final layer of weights and neural network output\n",
    "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((n_hidden, 1)), \n",
    "                                          jnp.ones((n_hidden, 1)))) \n",
    "    b3 = numpyro.sample(\"b3\", dist.Normal(jnp.zeros((1,)), \n",
    "                                          jnp.ones((1,))))  # D_H D_Y\n",
    "    z3 = jnp.matmul(z2, w3) + b3  # N D_Y  <= output of the neural network\n",
    "\n",
    "    # we put a prior on the observation noise\n",
    "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
    "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
    "\n",
    "    # observe data\n",
    "    numpyro.sample(\"obs\", dist.Normal(z3, sigma_obs), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n",
    "\n",
    "numpyro.set_host_device_count(2)\n",
    "kernel = NUTS(model)\n",
    "mcmc = MCMC(kernel, 100, 1000, num_chains=2, progress_bar=True)\n",
    "mcmc.run(rng_key, x.reshape(-1,1), y.reshape(-1,1), 10)\n",
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc.get_samples()\n",
    "\n",
    "def predict(model, rng_key, samples, X, D_H):\n",
    "    model = numpyro.handlers.substitute(numpyro.handlers.seed(model, rng_key), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = numpyro.handlers.trace(model).get_trace(x=X, y=None, n_hidden=D_H)\n",
    "    return model_trace['obs']['value']\n",
    "\n",
    "from jax import vmap\n",
    "vmap_args = (samples, random.split(rng_key, 1000 * 2))\n",
    "predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, x_test.reshape(-1,1), 10))(*vmap_args)\n",
    "predictions = predictions[..., 0]\n",
    "\n",
    "# compute mean prediction and confidence interval around median\n",
    "mean_prediction = jnp.mean(predictions, axis=0)\n",
    "percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test, mean_prediction)\n",
    "plt.fill_between(x_test, percentiles[0, :], percentiles[1, :], alpha=0.5)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://petuum.com/2019/01/15/intro-to-modern-bayesian-learning-and-probabilistic-programming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/pyro-ppl/numpyro/blob/master/examples/bnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
