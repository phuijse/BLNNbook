{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (brief) tutorial on [Pyro](https://pyro.ai/)\n",
    "\n",
    "Pyro can be used to perform MCMC and/or approximate inference for intractable posteriors\n",
    "\n",
    "We can use Pyro to move from point estimates to posteriors in our **torch-based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "display(pyro.__version__)\n",
    "pyro.set_rng_seed(12345) # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions and random variables in pyro\n",
    "\n",
    "Distributions in Pyro are implemented in [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "Let's start by creating a normal distribution. The `Normal` object expects its location $\\mu$ and scale $\\sigma$ as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal\n",
    "\n",
    "w_prior = Normal(loc=torch.tensor(0.), \n",
    "                 scale=torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw random sample from this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1000 samples \n",
    "samples = w_prior.rsample(sample_shape=(1000, )) # Reparameterized sampling\n",
    "display(samples.shape)\n",
    "\n",
    "# Build an histogram\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "plt.hist(samples.detach().numpy(), bins=20, density=True)\n",
    "# Plot the pdf\n",
    "w_plot = np.linspace(-3, 3, num=100)\n",
    "w_pdf = torch.exp(w_prior.log_prob(torch.from_numpy(w_plot))).detach().numpy()\n",
    "plt.plot(w_plot, w_pdf, 'k-', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute statistical descriptors from the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean\n",
    "display(w_prior.mean)\n",
    "#standard deviation\n",
    "display(w_prior.stddev)\n",
    "#entropy\n",
    "display(w_prior.entropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on the `shape` of pyro tensors**\n",
    "\n",
    "In `pytorch` the shape of a tensor is a tuple with the size of each of its dimensions\n",
    "\n",
    "In comparison `pyro` distributions have two shapes\n",
    "\n",
    "- `event_shape` refers to the dimensionality of the distribution, e.g. normal (number), multivariate normal (vector), Cholesky (matrix), etc. Is used to denote dependent random variables\n",
    "- `batch_shape` refers to a batch of distributions. Is used to denote conditionally independent random variables\n",
    "\n",
    "The \"final shape\" of a tensor sampled from a `pyro` distribution will be a combination of both\n",
    "\n",
    "In most cases we will assume independence between observations (batch dimension) and depedence between our model parameters (event dimension)\n",
    "\n",
    "Distributions have an attribute called `to_event` which converts indepedent dimensions to dependent ones\n",
    "\n",
    "**Example**\n",
    "\n",
    "The following tells `pyro` that the two components of this bivariate normal distribution are independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two conditionally independent normal RVs\n",
    "w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]]))\n",
    "display(w_prior.batch_shape, \n",
    "        w_prior.event_shape,\n",
    "        w_prior.rsample().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tells pyro to expect dependence between the components. We can interprete it as a multivariate normal with diagonal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]])).to_event(1)\n",
    "\n",
    "display(w_prior.batch_shape, \n",
    "        w_prior.event_shape,\n",
    "        w_prior.rsample().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random variables in Pyro**\n",
    "\n",
    "To create random variables that we can track within a model we use the [`pyro.sample`](http://pyro.ai/examples/intro_part_i.html#The-pyro.sample-Primitive) primitive\n",
    "\n",
    "`pyro.sample` expects a name and an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "For example, to create a variable named \"w\" with the previously defined distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w \\sim &\\mathcal{N}(\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&\\mu = \\begin{pmatrix}0 \\\\ 2 \\end{pmatrix}, \\sigma = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we would write the generative model as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    return pyro.sample(name='w', fn=w_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each time we run `model` a random sample is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression in pyro\n",
    "\n",
    "Let's consider the linear regression problem\n",
    "\n",
    "$$\n",
    "y_i = f_\\theta(x_i) + \\epsilon = w x_i + b + \\epsilon, \\forall i\n",
    "$$\n",
    "\n",
    "with our previous assumptions\n",
    "\n",
    "- $w$ and $b$ are random variables with normal priors\n",
    "- $y$ is a random variable with normal distribution (likelihood)\n",
    "- $x$ is a deterministic variable\n",
    "- We have $N$ tuples $(x,y)$\n",
    "\n",
    "The generative process is then\n",
    "\n",
    "1. Choose hyperparameters: $\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\sigma_\\epsilon$\n",
    "1. Sample: $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$\n",
    "1. Sample: $b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$\n",
    "1. For each $i=1,2,\\ldots, N$\n",
    "    1. Sample: $y_i \\sim \\mathcal{N}(w x_i + b, \\sigma_\\epsilon^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is often summarized using plate notation diagrams\n",
    "\n",
    "<img src=\"images/linear_regression_plate.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding the model**\n",
    "\n",
    "There are two ways to code the generative model in `pyro`\n",
    "\n",
    "The first is to use an unconditioned model and the pass the data with  `pyro.condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.0))\n",
    "\n",
    "def conditioned_model(x, y):\n",
    "    return pyro.condition(model, data={\"y\": y})(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is more direct. A random variable acting as the likelihood can be given the data through its `obs` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Uniform\n",
    "\n",
    "def model_obs(x, y=None):  \n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    # s = pyro.sample(\"s\", Uniform(0.01, 10.0))    \n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases the primitive [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate) is used for vectorized conditioning with the whole dataset \n",
    "\n",
    "The plate expects a name and the size of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling based Inference: MCMC\n",
    "\n",
    "With the model set we move on inferring the posterior of its parameters and predictions using `pyro`\n",
    "\n",
    "First we will review Markov Chain Monte Carlo (MCMC) to perform sampling based inference. MCMC methods return samples from a markov chain that converges to the posterior we care. We will focus on the practical implementation of MCMC using `pyro`. For more deep theoretical details on MCMC see Barber Chapter 27, [here](https://github.com/magister-informatica-uach/INFO337/tree/master/MCMC) or [here](https://github.com/phuijse/INFO274)\n",
    "\n",
    "The main wrapper for [MCMC methods in pyro](https://docs.pyro.ai/en/stable/mcmc.html) is \n",
    "\n",
    "```python\n",
    "pyro.infer.MCMC(kernel, # A sampler algoritm that decides the transitions\n",
    "                num_samples, # Number of samples excluding the warmup ones\n",
    "                warmup_steps=None, # Samples to discard in the beginning\n",
    "                initial_params=None, # If not specified they are sampled from the prior\n",
    "                num_chains=1, # Can run parallel chains\n",
    "                disable_validation=True, # Divergent transitions checks\n",
    "                ... # Only the most critical arguments are shown here\n",
    "               )\n",
    "``` \n",
    "\n",
    "The main methods of MCMC are\n",
    "\n",
    "- `run()`: Populates the chain, expects the same arguments as `model`\n",
    "- `diagnostics()`: Gives useful metrics to assess convergence\n",
    "- `summary()`: Returns a table with the statistics of the model parameters\n",
    "- `get_sample()`: Returns the posterior samples\n",
    "\n",
    "The currently available kernels are `HMC` (Hamiltonian Monte Carlo) and `NUTS` (No-U turn sampler). Both can deal with continous parameters. NUTS sets its step size automatically and is currently the state of the art. All kernels expects the function that specifies generative model plus their own particular arguments\n",
    "\n",
    "Let's run MCMC with NUTS for the Here we run MCMC as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "sampler = MCMC(kernel=NUTS(model_obs, adapt_step_size=True), \n",
    "               num_chains=1, num_samples=1000, warmup_steps=100)\n",
    "\n",
    "# Create some data and run the chain\n",
    "x = torch.tensor([-2., 2.])\n",
    "y = x\n",
    "# x = torch.randn(10)\n",
    "# y = 2*x -1 + 0.5*torch.randn(len(x))\n",
    "sampler.run(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the statistical moments, $\\hat r$ statistic and 90% credibility interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.summary(prob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the samples and visualize the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_samples()\n",
    "\n",
    "figure = corner.corner(np.stack((samples['b'].detach().numpy() , \n",
    "                                 samples['w'].detach().numpy() )).T, \n",
    "                       smooth=1., bins=20, quantiles=[0.16, 0.5, 0.84], \n",
    "                       labels=[\"b\", \"w\"], show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the posterior predictive of $y$ given $x$ we can use the utility class [`pyro.infer.Predictive`](http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.predictive.Predictive)\n",
    "\n",
    "```python\n",
    "pyro.infer.Predictive(model, # Model function\n",
    "                      num_samples=None, # How many samples to draw\n",
    "                      return_sites=(), # Which parameters to draw\n",
    "                      parallel=False, # Vectorized sampling\n",
    "                      posterior_samples=None, # Samples from the Markov Chain\n",
    "                      guide=None # Omit this for the moment\n",
    "                     )\n",
    "```\n",
    "\n",
    "If `posterior_samples` is provided then `samples.num_samples` will be generated regardless of what is specified by the `num_samples` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object\n",
    "predictive = pyro.infer.Predictive(model_obs, \n",
    "                                   return_sites=(\"w\", \"b\", \"y\"),\n",
    "                                   posterior_samples=sampler.get_samples())\n",
    "\n",
    "# Evaluate on test data\n",
    "x_test = np.linspace(-5, 5, num=100).astype('float32') \n",
    "predictive_samples = predictive(torch.from_numpy(x_test))\n",
    "\n",
    "# Plot it\n",
    "med = predictive_samples[\"y\"].median(axis=0).values.numpy()\n",
    "qua = predictive_samples[\"y\"].quantile(torch.tensor([0.05, 0.95]), axis=0).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(x_test, med)\n",
    "ax.fill_between(x_test, qua[0], qua[1], alpha=0.5);\n",
    "\n",
    "ax.errorbar(x.numpy(), y.numpy(), xerr=0, \n",
    "            yerr=2,\n",
    "            #yerr=2*samples['s'].mean().detach().numpy(), \n",
    "            fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**\n",
    "\n",
    "- Add a prior to $\\sigma_\\epsilon$\n",
    "- Train the chain with more data\n",
    "- Add hyperpriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
