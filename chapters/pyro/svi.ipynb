{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "display(pyro.__version__)\n",
    "pyro.set_rng_seed(12345) # For reproducibility\n",
    "x = torch.randn(5)\n",
    "y = 2*x -1 + 0.5*torch.randn(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Variational Inference with  [Pyro](https://pyro.ai/)\n",
    "\n",
    "Previously we learned \n",
    "\n",
    "- the fundamental ideas of variational inference\n",
    "- how to write probabilistic probabilistic models in `pyro`\n",
    "\n",
    "In this lesson we will continue our `pyro` tutorial focused on performing [Stochastic Variational inference](https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf)(SVI). \n",
    "\n",
    "SVI allows variational inference (VI) to scale to large databases by subsampling, i.e. estimating quantities of interest using minibatches of data. Typically we will be interested in first-order methods based on gradient descent to optimize our cost functions: Stochastic gradient descent (SGD). Basically\n",
    "\n",
    "> VI + SGD = SVI\n",
    "\n",
    "The unified interface for SVI in `pyro` is located in [`pyro.infer.SVI`](http://docs.pyro.ai/en/stable/inference_algos.html). The parameters of the SVI object are\n",
    "\n",
    "```python\n",
    "pyro.infer.SVI(model, # A function that defines our generative model \n",
    "               guide, # A function that defines our approximate posterior\n",
    "               optim, # A \"gradient-descent-based\" optimizer\n",
    "               loss, # The cost function: Some variant of the ELBO\n",
    "               ...\n",
    ")\n",
    "```\n",
    "\n",
    "We already saw how to write models. Now we will focus now on how to write guides using the Bayesian linear regression from the previous pyro lesson as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "def model_obs(x, y=None):  \n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    s = pyro.sample(\"s\", Uniform(0.01, 10.0))    \n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, s), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide function\n",
    "\n",
    "The guide represents our approximate posterior $q_\\nu(\\theta)$, i.e. it has to specify the distribution for the posterior of the parameters of the model. \n",
    "\n",
    "More technically\n",
    "\n",
    "- Every `pyro.sample` in the model that represents a latent variable has to be in the guide (using the same name)\n",
    "- The primitive [`pyro.param`]() is used to register the hyperparameters of these latent variables\n",
    "- The arguments of the guide have to be the same as those in the model\n",
    "- The function does not need to return anything\n",
    "\n",
    "The registered parameters correspond to $\\nu$. These are the values that we will optimize later\n",
    "\n",
    "For the bayesian linear regression $\\theta = (w, b)$. We will create a fully factored normal model\n",
    "\n",
    "$$\n",
    "q_\\nu(w,b,s) = q_{\\nu_w}(w)q_{\\nu_b}(b) q_{\\nu_s}(s) = \\mathcal{N}(w|\\mu_w, \\sigma_w^2) \\mathcal{N}(b|\\mu_b, \\sigma_b^2) \\mathcal{N}(s|\\mu_s, 0.05)\n",
    "$$\n",
    "\n",
    "The hyperparameters are $\\nu = (\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\mu_s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "\n",
    "def guide(x, y=None): \n",
    "    # slope\n",
    "    w_loc = pyro.param(\"w_loc\", torch.tensor(0.))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", Normal(w_loc, w_scale))\n",
    "    # intercept\n",
    "    b_loc = pyro.param(\"b_loc\", torch.tensor(0.))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    b = pyro.sample(\"b\", Normal(b_loc, b_scale))\n",
    "    # noise variance\n",
    "    s_loc = pyro.param(\"s_loc\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    s = pyro.sample(\"s\", Normal(s_loc, torch.tensor(0.05)), constraint=constraints.positive)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "In the previous lesson we studied the Evidence Lower Bound (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\nu &= \\text{arg}\\max_\\nu \\mathcal{L}(\\nu) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_\\nu \\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}|\\theta) p (\\theta)}{q_\\nu(\\theta)} d\\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- The model function defines $p(\\mathcal{D}|\\theta) p (\\theta)$ \n",
    "- The guide function defines $q_\\nu(\\theta)$ \n",
    "\n",
    "Pyro offers several versions of the [ELBO](https://docs.pyro.ai/en/stable/inference_algos.html#module-pyro.infer.elbo)\n",
    "\n",
    "- `Trace_ELBO`: Default ELBO. Reduces variance of the gradients using \"Rao-Blackwellization\"\n",
    "- `TraceEnum_ELBO`: Performs exhaustive enumeration for discrete variables\n",
    "- `TraceMeanField_ELBO`: Assumes Mean-field structure. Reduce variance of gradients using analytical KL when possible\n",
    "\n",
    "We will study the importance of gradient variance later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The main method of the SVI object is \n",
    "\n",
    "- `svi.step(*args)`: Performs a gradient step, similar to the `backward()` plus `step()` in pytorch\n",
    "\n",
    "The `step()` method receives the inputs for guide and model as arguments\n",
    "\n",
    "In this example we select the default ELBO and SGD with adaptive learning rate as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Activate additional debug of model/guides\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "svi = pyro.infer.SVI(model=model_obs,  \n",
    "                     guide=guide,                     \n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 0.01}),# Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO(num_particles=10,\n",
    "                                                vectorize_particles=True) # Loss function\n",
    "                    ) \n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(10, 2.5), dpi=80, tight_layout=True)\n",
    "lines = [ax_.plot([], [])[0] for ax_ in ax]\n",
    "param_names = [\"ELBO\", \"w_loc\", \"b_loc\", \"s_loc\", \"w_scale\", \"b_scale\"]\n",
    "param_evolution = {}\n",
    "for name in param_names:\n",
    "    param_evolution[name] = []\n",
    "    \n",
    "for ax_, name in zip(ax, param_names):\n",
    "    ax_.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(range(3000)):\n",
    "    param_evolution[\"ELBO\"].append(svi.step(x, y))\n",
    "    for name in param_names[1:]:\n",
    "        param_evolution[name].append(pyro.param(name).item()) \n",
    "    \n",
    "    if np.mod(k, 100) == 0:\n",
    "        for i, name in enumerate(param_names):\n",
    "            lines[i].set_ydata(param_evolution[name][:k])\n",
    "        for line in lines:\n",
    "            line.set_xdata(range(k))\n",
    "        for ax_ in ax.ravel():\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model_obs, \n",
    "                                   return_sites=(\"w\", \"b\", \"s\", \"y\"),\n",
    "                                   guide=guide,\n",
    "                                   num_samples=1000\n",
    "                                   #posterior_samples=sampler.get_samples()\n",
    "                                   )\n",
    "\n",
    "x_test = np.linspace(-5, 5, num=100).astype('float32') \n",
    "predictive_samples = predictive(torch.from_numpy(x_test))\n",
    "med = predictive_samples[\"y\"].median(axis=0).values.numpy()\n",
    "qua = predictive_samples[\"y\"].quantile(torch.tensor([0.05, 0.95]), axis=0).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(x_test, med)\n",
    "ax.fill_between(x_test, qua[0], qua[1], alpha=0.5);\n",
    "\n",
    "ax.errorbar(x.numpy(), y.numpy(), xerr=0, \n",
    "            yerr=predictive_samples[\"s\"].median().numpy(),\n",
    "            fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the predictive posterior of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "figure = corner.corner(np.stack((predictive_samples['b'].detach().numpy() , \n",
    "                                 predictive_samples['w'].detach().numpy(),\n",
    "                                 predictive_samples['s'].detach().numpy())).T, \n",
    "                       smooth=1., bins=20, quantiles=[0.16, 0.5, 0.84], \n",
    "                       labels=[\"b\", \"w\", \"s\"], show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get the name and value of the hyper-parameters using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoguides\n",
    "\n",
    "Given a model, `pyro` offers functions to generate guides automatically from it. These are found in the [`pyro.infer.autoguide`](https://docs.pyro.ai/en/stable/infer.autoguide.html) module\n",
    "\n",
    "For example the guide we previously wrote is roughly equivalent to `AutoDiagonalNormal`, a guide where the latent variables are normal and independent\n",
    "\n",
    "Other interesting auto guides are\n",
    "\n",
    "- `AutoMultivariateNormal`: Adds a correlation matrix for the latent variables \n",
    "- `AutoLowRankMultivariateNormal`: Similar to the previous one, but with a low rank covariance\n",
    "- `AutoDelta`: Returns the MAP\n",
    "- `AutoLaplaceApproximation`: Multivariate normal guide centered on the MAP with variance equal to the neg hessian\n",
    "- `AutoNormalizingFlow`: Uses a sequence of bijective transformations starting from a Gaussian to obtain a more flexible distribution (more in this in future lessons)\n",
    "\n",
    "Let's test some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDelta, AutoLaplaceApproximation, AutoDiagonalNormal, AutoMultivariateNormal\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "#guide = AutoDelta(model_obs)\n",
    "#guide = AutoDiagonalNormal(model_obs, init_scale=1e-2)\n",
    "guide = AutoMultivariateNormal(model_obs, init_scale=1e-2)\n",
    "\n",
    "svi = pyro.infer.SVI(model=model_obs,  \n",
    "                     guide=guide,                     \n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 0.01}),# Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO(num_particles=10,\n",
    "                                                vectorize_particles=True) # Loss function\n",
    "                    ) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ELBO = []\n",
    "\n",
    "for k in tqdm(range(3000)):\n",
    "    ELBO.append(svi.step(x, y))\n",
    "    \n",
    "    if np.mod(k, 100) == 0:\n",
    "        ax.cla()\n",
    "        ax.plot(ELBO)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model_obs, \n",
    "                                   return_sites=(\"w\", \"b\", \"s\", \"y\"),\n",
    "                                   guide=guide,\n",
    "                                   num_samples=1000)\n",
    "x_test = np.linspace(-5, 5, num=100).astype('float32') \n",
    "predictive_samples = predictive(torch.from_numpy(x_test))\n",
    "\n",
    "import corner\n",
    "figure = corner.corner(np.stack((predictive_samples['b'].detach().numpy() , \n",
    "                                 predictive_samples['w'].detach().numpy(),\n",
    "                                 predictive_samples['s'].detach().numpy())).T, \n",
    "                       smooth=1., bins=20, quantiles=[0.16, 0.5, 0.84], \n",
    "                       labels=[\"b\", \"w\", \"s\"], show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study\n",
    "\n",
    "- Pyro's SVI tutorial part [I](https://pyro.ai/examples/svi_part_i.html) and [II](https://pyro.ai/examples/svi_part_ii.html)\n",
    "- [Pyro tutorial tips and tricks](https://pyro.ai/examples/svi_part_iv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Do example on [correlated gaussian](https://jmhldotorg.files.wordpress.com/2013/11/slidescharlesuniversitylaplacevi2013.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
