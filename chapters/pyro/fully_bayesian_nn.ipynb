{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import corner\n",
    "import torch\n",
    "import pyro\n",
    "display(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "Deep Neural Networks are non-linear function approximators which represent the state of the art in pattern recognition\n",
    "\n",
    "But they do have limitations\n",
    "\n",
    "- Very deep models require lots of data to train\n",
    "- Selecting an architecture requires a lot of experimentation\n",
    "- [Easily ](https://arxiv.org/abs/1412.1897) [fooled](https://openai.com/blog/adversarial-example-research/)\n",
    "- Poor at representing uncertainty \n",
    "\n",
    "> We can address some of these limitations by going Bayesian\n",
    "\n",
    "A Bayesian neural network (BNN) places a prior distribution on its parameters. Training the BNN is equivalent to learning the posterior distribution of the parameters given the data. Most importantly the **uncertainty on the data and the parameters** can be propagated to estimate the **uncertainty on our predictions**\n",
    "\n",
    "- Uncertainty on the data is called **aleatoric uncertainty** and it is related to irreducible noise\n",
    "- Uncertainty on the model (parameters and structure) is called **epistemic uncertainty**\n",
    "\n",
    "> BNN's (and other bayesian models) know what they don't know\n",
    "\n",
    "We can use this \"new knowledge\" to\n",
    "\n",
    "- Choose when to use a more simple/complex model (complexity-control)\n",
    "- Make critical decisions, e.g. [autonomous cars](https://en.wikipedia.org/wiki/Tesla_Autopilot#Non-fatal_crashes), cancer diagnosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of history\n",
    "\n",
    "- 1980's: Bayes theorem is applied to Neural Networks (John Hopfield and Naftali Tishby)\n",
    "- 1990's: Monte-Carlo and VI for bayesian neural networks was studied extensively by [David Mackay](http://www.inference.org.uk/mackay/BayesNets.html) and [Radford Neal](https://www.cs.toronto.edu/~radford/res-neural.html) (Also Bishop, Barber, Hinton, Gharamani and many others). Neal shows that Gaussian process are bayesian neural networks with infinite neurons\n",
    "- 2011: [Alex Graves' VI for neural networks](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks). Explosion of practical deep bayesian networks \n",
    "    - [Charles Blundell's Bayes by backprop](https://arxiv.org/abs/1505.05424)\n",
    "    - [Yarin Gal's many work](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)\n",
    "    - Durk Kingma, Danilo Jimenez Rezende, Shakir Mohamed, JosÃ© Miguel Hernandez-Lobato\n",
    "- [Hot topic now a days](http://bayesiandeeplearning.org/)\n",
    "\n",
    "History in video by [Zoubin Gharamani](http://mlg.eng.cam.ac.uk/zoubin/) at [NIPS 2016](https://www.youtube.com/watch?v=FD8l2vPU5FY) and [interesting panel discussion](https://www.youtube.com/watch?v=HumFmLu3CJ8) on the same conference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalism recap\n",
    "\n",
    "Assuming\n",
    "\n",
    "- $N$ *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}$ \n",
    "- $x$ is a $D$ dimensional vector, $y$ is a scalar\n",
    "- Fully-connected neural network with one hidden layer ($H$ neurons) for regression\n",
    "- $\\text{tanh}(\\cdot)$ as non-linear activation function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\theta(x) &=   \\hat b + \\sum_{j=1}^H \\hat w_{j} h_j  \\nonumber \\\\\n",
    "&=  \\hat b + \\sum_{j=1}^H \\hat w_{j} \\text{tanh} \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The parameter vector $\\theta = (b, w, \\hat b, \\hat w)$ contains all the weights and biases of the model\n",
    "\n",
    "**Prior:** We propose a prior for $\\theta$, typically\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "**Likelihood:** We propose a likelihood depending on our task, typically Gaussian for regression and Bernoulli/Categorical for binary/multiclass classification \n",
    "\n",
    "**Posterior:** We use Bayes theorem to write the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{1}{{p(\\mathcal{D})}} \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "Even though the likelihood and prior are normal **the posterior in this case is not normal** because of the nested nonlinearity \n",
    "\n",
    "In general:\n",
    "\n",
    "> We cannot obtain an analytical posterior for a bayesian neural network\n",
    "\n",
    "We resort to sampling-based (MCMC) or deterministic (VI) approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first Bayesian Neural Network using `pyro`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same synthetic data from the linear regression lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding the bayesian neural net**\n",
    "\n",
    "Neural networks in `pyro` are classes that inherit from [`pyro.nn.PyroModule`](https://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule) which is a subclass of `torch.nn.Module`\n",
    "\n",
    "Within the `PyroModule` defined model we use\n",
    "\n",
    "- `PyroSample` to declare random variable, e.g. weights and biases\n",
    "- `PyroParam` to declare deterministic parameters, e.g. the parameters of the priors\n",
    "- `PyroModule` to declare torch modules which accept random parameters\n",
    "\n",
    "In the following example we lift `torch.nn.Linear` using `PyroModule`, and add priors to its parameters using `PyroSample`\n",
    "\n",
    "In this regression problem we assume that the output is Gaussian distributed. The likelihood is declared with its corresponding plate in the `forward` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "import pyro.distributions as dists \n",
    "\n",
    "class BayesianMLPRegression(PyroModule):\n",
    "    def __init__(self, n_hidden=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "        prior = dists.Normal(0, prior_scale)\n",
    "        # Hidden layer\n",
    "        self.hidden = PyroModule[torch.nn.Linear](1, n_hidden)\n",
    "        self.hidden.weight = PyroSample(prior.expand([n_hidden, 1]).to_event(2))\n",
    "        self.hidden.bias = PyroSample(prior.expand([n_hidden]).to_event(1))\n",
    "        # Output layer\n",
    "        self.output = PyroModule[torch.nn.Linear](n_hidden, 1)\n",
    "        self.output.weight = PyroSample(prior.expand([1, n_hidden]).to_event(2))\n",
    "        self.output.bias = PyroSample(prior.expand([1]).to_event(1))\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        f = self.output(z).squeeze(-1)            \n",
    "        #sigma = pyro.sample(\"sigma\", dists.Uniform(0.0, 0.1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            loc = pyro.deterministic(\"mean\", f, event_dim=0)   \n",
    "            obs = pyro.sample(\"obs\", dists.Normal(loc, 0.1), obs=y) #likelihood\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is coded we can use `pyro.poutine.trace` with pyro validation activated to make sure that the shapes are correct\n",
    "\n",
    "- Batch dimension is 15 (number of samples)\n",
    "- Event dimension is equal to the number of neurons for each layer\n",
    "\n",
    "Independent RV (likelihood) should be in the left while dependent (weights and biases) should be on the right\n",
    "\n",
    "This is controlled using plates and the `to_event()` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "\n",
    "model = BayesianMLPRegression()\n",
    "\n",
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the BNN: MCMC** \n",
    "\n",
    "We could train this model using MCMC as seen before\n",
    "\n",
    "```python\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "nuts_kernel = NUTS(model, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=2, num_samples=1000, warmup_steps=100)\n",
    "sampler.run(x_torch, y_torch)\n",
    "```\n",
    "\n",
    "But even for a extremely simple BNN and using the most advanced samplers MCMC can be inpractical. Note that this may change in the future with projects such as [NumPyro](https://github.com/pyro-ppl/numpyro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of VI\n",
    "\n",
    "We propose an approximate (simple) posterior $q_\\nu(\\theta)$ and optimize so that it looks similar to the actual posterior\n",
    "\n",
    "We do this by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\nu) = \\mathbb{E}_{q_\\nu(\\theta)}[ \\log p(\\mathcal{D}|\\theta)] - \\text{KL}[q_\\nu(\\theta)|p(\\theta)]\n",
    "$$\n",
    "\n",
    "Then we use $q_\\nu(\\theta)$ as our replacement for $p(\\theta|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, \\theta) p(\\theta| \\mathcal{D}) \\,d\\theta\n",
    "$$\n",
    "\n",
    "In what follows we will see how to do this for the BNN using pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the BNN: VI**\n",
    "\n",
    "Once the model is specified we need to write a guide (approximate posterior). This can be done manually or using the automatic guides in `pyro.infer.autoguide`. Typically we would start with the simplest diagonal normal guide that assumes no correlation between the parameters of the BNN\n",
    "\n",
    "Then we create an SVI object and call the `step` attribute of this object iteratively. We can evaluate the posteriors of the parameters and the predictive posterior using `pyro.infer.Predictive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True, dpi=80)\n",
    "\n",
    "def update_plot(k, epoch_loss, samples):\n",
    "    ax[0].cla()\n",
    "    ax[0].plot(range(k), epoch_loss[:k])\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_ylabel('ELBO')\n",
    "    ax[1].cla()\n",
    "    ax[1].plot(x, y, 'k.');\n",
    "    med = np.median(samples, axis=[0])\n",
    "    qua = np.quantile(samples, (0.05, 0.95), axis=0)\n",
    "    ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "    ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows the neural network is trained for 1000 epochs and every 10 epochs the predictive posterior is plotted. The `mean` site is plotted (model uncertainty). To observe model plus data uncertainty plot the `obs` site\n",
    "\n",
    "Note that the scale of the prior, the scale of the likelihood and the initial scale of the approximate posterior are sensible parameters\n",
    "\n",
    "This is of course in in addition to the number of hidden units and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this on for additional debugging\n",
    "pyro.enable_validation(True) \n",
    "pyro.set_rng_seed(123)\n",
    "pyro.clear_param_store() \n",
    "# Declare the neural network\n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=10) \n",
    "\n",
    "# Create a guide\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-2)\n",
    "\n",
    "# Create SVI object\n",
    "svi = pyro.infer.SVI(model, guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2, 'clip_norm': 10.0}), # Optimizer\n",
    "                     loss=pyro.infer.TraceMeanField_ELBO(num_particles=1)) # Loss function \n",
    "\n",
    "epoch_loss = np.zeros(shape=(1000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch) # Actual training step\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 10 == 0:\n",
    "        # Compute predictive posterior\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "        samples = predictive(x_test, None)['mean'].detach().numpy()\n",
    "        # Plot it\n",
    "        update_plot(k, epoch_loss, samples)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete we can use the guide as our replacement to the posterior\n",
    "\n",
    "The trained pararemeters of the guide are stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can use `pyro.infer.Predictive` to get samples from our bayesian neural network when evaluated on new inputs \n",
    "\n",
    "Here we sample \"100 neural networks\" and evaluate them on `x_test` \n",
    "\n",
    "This returns the sampled parameters (weights and biases) and outputs (obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "for k, v in predictive(x_test, None).items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True, dpi=80)\n",
    "ax.plot(x_test, predictive(x_test)['mean'].detach().numpy().T, c='b', alpha=0.1);\n",
    "ax.plot(x, y, 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian network for multi-class classification with `pyro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create synthetic 2D data with 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='int') # class labels\n",
    "\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0, 0.5, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "\n",
    "#X, y = sklearn.datasets.make_moons(200, noise=0.2)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, alpha=0.5);\n",
    "\n",
    "idx = np.random.permutation(N*3)\n",
    "train_prop = 0.7\n",
    "x_train = torch.from_numpy(X[idx[:int(N*K*train_prop)]].astype('float32'))\n",
    "y_train = torch.from_numpy(y[idx[:int(N*K*train_prop)]])\n",
    "x_test = torch.from_numpy(X[idx[int(N*K*train_prop):]].astype('float32'))\n",
    "y_test = torch.from_numpy(y[idx[int(N*K*train_prop):]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an implementation of a Bayesian Neural Network with two hidden layers and normal prior in all activations\n",
    "\n",
    "For the likelihood we use the Categorical (Multinomial with $n=1$). The categorical distribution expects unnormalized probabilities (logits) as input, in this case the un-activated output of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Normal, Categorical\n",
    "\n",
    "class BayesianMLPClassifier(PyroModule):\n",
    "    def __init__(self, num_hidden=10, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_std)\n",
    "        self.layer1 = PyroModule[torch.nn.Linear](2, num_hidden)\n",
    "        self.layer1.weight = PyroSample(prior.expand([num_hidden, 2]).to_event(2))\n",
    "        self.layer1.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        #self.layer2 = PyroModule[torch.nn.Linear](num_hidden, num_hidden)\n",
    "        #self.layer2.weight = PyroSample(prior.expand([num_hidden, num_hidden]).to_event(2))\n",
    "        #self.layer2.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer3 = PyroModule[torch.nn.Linear](num_hidden, 3)\n",
    "        self.layer3.weight = PyroSample(prior.expand([3, num_hidden]).to_event(2))\n",
    "        self.layer3.bias = PyroSample(prior.expand([3]).to_event(1))        \n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.layer1(x))\n",
    "        #h = self.activation(self.layer2(h))\n",
    "        f = self.layer3(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            logp = pyro.deterministic(\"logp\", f, event_dim=1)\n",
    "            obs = pyro.sample(\"obs\", Categorical(logits=logp), obs=y) # Multiclass\n",
    "            #obs = pyro.sample(\"obs\", dist.Bernoulli(logits=p), obs=y) # Binary\n",
    "        return f\n",
    "    \n",
    "    \n",
    "#pyro.enable_validation(True)\n",
    "#model = BayesianMLPClassifier()\n",
    "#print(pyro.poutine.trace(model).get_trace(x_train, y_train).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use an automatic diagonal normal guide (no covariance) and train using `Trace_ELBO`\n",
    "\n",
    "We plot the mean of the predictive posterior every 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, samples):\n",
    "    ax[0].cla()\n",
    "    p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "    zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "    ax[0].pcolormesh(xx, yy, zz, shading='auto', cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    \n",
    "\n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    ax[1].relim()\n",
    "    ax[1].autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(123)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianMLPClassifier(num_hidden=100, prior_std=10.)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-1)\n",
    "\n",
    "svi = pyro.infer.SVI(model, guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.TraceMeanField_ELBO())\n",
    "\n",
    "epoch_loss = np.zeros(shape=(3000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = svi.step(x_train, y_train)\n",
    "    if k % 100 == 0:\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=10)\n",
    "        samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "        update_plot(k, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 100 neural networks and plot four individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(9, 2), tight_layout=True)\n",
    "for k in range(4):\n",
    "    zz = samples[\"obs\"][k].reshape(xx.shape).detach().numpy()\n",
    "    ax[k].pcolormesh(xx, yy, zz, shading='auto', cmap=plt.cm.Set1)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[k].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these categorical samples we can compute statistics\n",
    "\n",
    "In the left we plot the mode (more repeated class) and in the right the entropy. \n",
    "\n",
    "The higher then entropy the more different the output of the neural networks (high uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "zz = torch.mode(samples[\"obs\"], dim=0)[0].reshape(xx.shape).detach().numpy()\n",
    "ax[0].pcolormesh(xx, yy, zz, shading='auto', cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)/100.\n",
    "entropy = lambda p: -(p*(p+1e-32).log()).sum(dim=1)\n",
    "\n",
    "zz = entropy(p).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75)\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result using a non-bayesian neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):    \n",
    "    def __init__(self, num_hidden=10):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, num_hidden) \n",
    "        #self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = torch.nn.Linear(num_hidden, 3)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.layer1(x))\n",
    "        #z = self.activation(self.layer2(z))\n",
    "        return self.layer3(z)     \n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, model):\n",
    "    ax[0].cla()\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    zz = torch.nn.Softmax(dim=1)(Z).argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "    ax[0].pcolormesh(xx, yy, zz, shading='auto', cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    ax[1].relim()\n",
    "    ax[1].autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(num_hidden=100)\n",
    "display(model)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(x, y, phase='train'):\n",
    "    haty = model.forward(x) # Evaluate the model\n",
    "    loss = criterion(haty, y) # Calculate errors\n",
    "    if phase == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Compute derivatives\n",
    "        optimizer.step() # Update parameters \n",
    "    return loss.item()\n",
    "\n",
    "x_train = torch.from_numpy(X.astype('float32'))#.reshape(-1, 1)\n",
    "y_train = torch.from_numpy(y)#.reshape(-1, 1)\n",
    "epoch_loss = np.zeros(shape=(3000,)) \n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = train_one_epoch(x_train, y_train)\n",
    "    if k % 100 == 0: \n",
    "        update_plot(k, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the softmax output as probabilities we can also compute its entropy\n",
    "\n",
    "Is it the same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Z = torch.nn.Softmax(dim=1)(model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))))\n",
    "zz = Z.argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "ax[0].pcolormesh(xx, yy, zz, shading='auto', cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "zz = -(Z*(Z+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75, vmin=0., vmax=np.log(3))\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is related to phenomenon of **uncertainty miscalibration in neural networks**, i.e. the uncertainty of the predictions tends to be very low even when far from the data\n",
    "\n",
    "> \"after (almost) all training samples are correctly classified, crossentropy (neg log likelihood) can be further minimized by increasing the confidence of the predictions\", *i.e.* reducing the entropy of softmax output\n",
    "\n",
    "The uncertainty obtained from model averaging (bayesian) and the one derived from the softmax output should not be confused\n",
    "\n",
    "Further reading and references on this topic:\n",
    "\n",
    "- [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)\n",
    "- [Being Bayesian, Even Just a Bit,Fixes Overconfidence in ReLU Networks](https://arxiv.org/pdf/2002.10118v1.pdf)\n",
    "- [Evidential Deep Learning to Quantify Classification Uncertainty](https://arxiv.org/pdf/1806.01768.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks on Bayesian Neural Networks Training\n",
    "\n",
    "- Actively research nowadays \n",
    "- Delicate: bad initializations, local minima, appropriate priors\n",
    "- Variance control and reparameterization (more on this next class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
