{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "from tqdm.notebook import tqdm\n",
    "print(f\"Pyro version: {pyro.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Gaussian processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Before training a neural network (deterministic or probabilistic) we have to **select its architecture**. \n",
    "\n",
    "For example, in the case of a multilayer perceptron (MLP), we need to choose the number of layers (depth) and the number of nodes (neurons) per layer (width)\n",
    "\n",
    "> With this we are defining the neural network as a **function with fixed structure**\n",
    "\n",
    "Increasing the width and depth gives the model more flexibility. But in general we don't know \"how much\" flexibility is needed for a particular problem\n",
    "\n",
    "The architecture is a collection of hyper-parameters. The good practice is to find the \"best structure\" using a validation (holdout) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of testing several architectures we could use a non-parametric model with no fixed structure\n",
    "\n",
    "In this lesson we will study the **Gaussian Process (GP)**, a bayesian non-parametric model that can be seen as a neural network with one hidden layer and potentially infinite width\n",
    "\n",
    "> In general non-parametric models automatically grow in complexity (width) with data\n",
    "\n",
    "Note that non-parametric models do have prior distributions and tunable hyper-parameters. The difference is that the distribution of its parameters lives in an infinite dimensional space. We use non-parametric models by integrating out (marginalizing) this infinite distribution\n",
    "\n",
    "Other non-parametric models not covered in this lesson are the many variants of the Dirichlet Process, the infinite Hidden Markov Model and the Indian Buffer Process. See Ghahramani's tutorial at the end of this lesson for a presentation of these methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorical background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gaussian process?\n",
    "\n",
    "Consider the probabilistic linear regression problem from previous lessons\n",
    "\n",
    "$$\n",
    "y_i =  \\sum_{k=1}^M \\phi_k(x_i) \\theta_k + \\epsilon_i \\quad \\forall i=1,2,\\ldots,N\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i$ is *iid* Gausian and $\\phi_k$ is a collection of $M$ (non-linear) basis functions\n",
    "\n",
    "We can write this in matrix form as\n",
    "\n",
    "$$\n",
    "Y = \\Phi(X) \\theta + E\n",
    "$$\n",
    "\n",
    "where $E$ is a diagonal matrix and $\\Phi(X) \\in \\mathbb{R}^{N \\times M}$\n",
    "\n",
    "The prior for $\\theta$ is Gaussian\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0,  \\sigma_\\theta^2 I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may ask \n",
    "\n",
    "> What is the distribution of $f_\\theta(X) = \\Phi(X) \\theta$ ?\n",
    "\n",
    "If $\\Phi$ is a deterministic transformation then the distribution of $f$ is also Gaussian \n",
    "\n",
    "By our previous definitions the mean of $p(f)$ is \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f_\\theta(X)] = \\Phi(X)\\mathbb{E}[\\theta] = 0\n",
    "$$\n",
    "\n",
    "and its covariance is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[ f_\\theta(X) f_\\theta(X)^T] =  \\Phi(X)\\mathbb{E}[\\theta \\theta^T ] \\Phi(X)^T = \\sigma_\\theta^2 \\Phi(X) \\Phi(X)^T = K\n",
    "$$\n",
    "\n",
    "where $K \\in \\mathbb{R}^{N\\times N}$ is a symmetric and positive-definite matrix called the **Gram matrix** or **Gramian matrix**.\n",
    "\n",
    "The $ij$-th element of the gram matrix is\n",
    "\n",
    "$$\n",
    "K_{ij} = \\sum_{k=1}^M \\phi_k(x_i) \\phi_k(x_j) = \\left \\langle \\vec \\phi(x_i) , \\vec \\phi(x_j) \\right \\rangle = \\kappa(x_i, x_j)\n",
    "$$\n",
    "\n",
    "where $\\kappa(\\cdot, \\cdot): \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is called the **kernel**. In general we will forget about $\\{\\phi_k(\\cdot)\\}$ and work only with the kernel (more about this later).\n",
    "\n",
    "With all this we can finally write\n",
    "\n",
    "$$\n",
    "p(f) = \\mathcal{N}(0, K)\n",
    "$$\n",
    "\n",
    "which is a \"prior over functions\". Note that we have dropped the dependence on $\\theta$\n",
    "\n",
    "We can say that $f$ is a multivariate random variable or \"random process\" with joint Gaussian distribution: $f$ is a **Gaussian Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we do inference with a GP?\n",
    "\n",
    "Let's say we have a dataset $\\textbf{x}=(x_1, x_2)$, $\\textbf{y}=(y_1, y_2)$ and we want to infer $f(x^*)$ or $f^*$ for short, i.e. we are interested in the posterior $p(f^*|\\textbf{y}, \\textbf{x}, x^*)$\n",
    "\n",
    "As before we can write the joint (Gaussian) distribution between the dataset and the new sample as\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}, f^*|\\textbf{x}, x^*) = \\mathcal{N}(0, K^+)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "K^+ = \\begin{pmatrix} K_{\\textbf{x}\\textbf{x}} + \\sigma_\\epsilon^2 I & K_{\\textbf{x}x^*} \\\\ K_{\\textbf{x}x^*}^T & K_{x^*x^*} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is a block matrix and\n",
    "\n",
    "$$\n",
    "K_{\\textbf{x}\\textbf{x}} = \\begin{pmatrix} \\kappa(x_1, x_1) & \\kappa(x_1, x_2) \\\\ \\kappa(x_1, x_2) & \\kappa(x_2, x_2)\\end{pmatrix}, \\quad K_{\\textbf{x}x^*} = \\begin{pmatrix} \\kappa(x_1, x^*) \\\\ \\kappa(x_2, x^*) \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The Gaussian distribution is closed under conditioning, i.e. if we have a joint gaussian distribution the conditional distribution of a variable given the others is gaussian ([nice step by step example](https://fabiandablander.com/statistics/Two-Properties.html))\n",
    "\n",
    "Here we will use this property to write \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(f^*|\\textbf{y}, \\textbf{x}, x^*) = \\mathcal{N}(&K_{\\textbf{x}x^*} (K_{\\textbf{x}\\textbf{x}}+I\\sigma_\\epsilon^2)^{-1} \\textbf{y}, \\nonumber \\\\\n",
    "& K_{x^*x^*} - K_{\\textbf{x}x^*} (K_{\\textbf{x}\\textbf{x}}+I\\sigma_\\epsilon^2)^{-1} K_{\\textbf{x}x^*}^T ) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which gives us the result we seek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gaussian conditioning to predict on several \"new data points\" at the same time, we only need to compute the sub gram matrices between and within the training set and the test set\n",
    "\n",
    "<img src=\"images/gram_matrix_block.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about the kernel\n",
    "\n",
    "The GP is mainly defined by its covariance also known as the gram matrix\n",
    "\n",
    "$$\n",
    "K = \\begin{pmatrix} \n",
    "\\kappa(x_1, x_1)& \\kappa(x_1, x_2)& \\ldots & \\kappa(x_1, x_N) \\\\\n",
    "\\kappa(x_2, x_1)& \\kappa(x_2, x_2)& \\ldots & \\kappa(x_2, x_N) \\\\\n",
    "\\vdots& \\vdots& \\ddots & \\vdots \\\\\n",
    "\\kappa(x_N, x_1)& \\kappa(x_N, x_2)& \\ldots & \\kappa(x_N, x_N) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the following relation between the kernel and the basis function\n",
    "\n",
    "$$\n",
    "\\kappa(x_i, x_j) = \\left \\langle \\vec \\phi(x_i) , \\vec \\phi(x_j) \\right \\rangle \n",
    "$$\n",
    "\n",
    "is known as the \"[kernel trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)\". \n",
    "\n",
    "Before we defined a finite dimensional $\\vec \\phi$ and obtained $\\kappa$. But in general it is more interesting to skip $\\phi$ and design $\\kappa$ directly. We only need $\\kappa$ to be a symmetric and positive-definite function. The broadly used Gaussian kernel complies with these restrictions\n",
    "\n",
    "$$\n",
    "\\kappa(x_i, x_j) = \\sigma^2 \\exp \\left ( \\frac{\\|x_i - x_j \\|^2}{2\\ell^2} \\right)\n",
    "$$\n",
    "\n",
    "where hyperparameter $\\sigma$ controls the amplitude and $\\ell$ controls the length-scale of the interactions between samples. \n",
    "\n",
    "Using a taylor expansion we can show that the (non-linear) basis function of this kernel is \n",
    "\n",
    "$$\n",
    "\\phi(x) = \\lim_{M\\to\\infty} \\sigma^2 \\exp\\left({-\\frac{\\|x\\|^2}{2\\ell^2}}\\right) \\begin{pmatrix} 1 & \\frac{x}{\\ell} & \\frac{x^2}{\\ell^2 \\sqrt{2}} & \\cdots & \\frac{x^M}{\\ell^M \\sqrt{M!}}  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "i.e. the Gaussian kernel induces an infinite-dimensional basis function. \n",
    "\n",
    "> A Gaussian process with Gaussian kernel has an implicit infinite dimensional parameter vector $\\theta$. \n",
    "\n",
    "We are not anymore explicitely choosing the structure of the function, but we selecting a kernel we are choosing a general \"behavior\". For example the Gaussian kernels encodes the property of locality, i.e. closer data samples should have similar predictions. \n",
    "\n",
    "Several other valid kernels exist that encode other properties such as trends and periodicity as exemplified in the following picture from Mackay's book\n",
    "\n",
    "<img src=\"images/kernels_mackay.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we train a GP?\n",
    "\n",
    "Fitting a GP to a dataset corresponds to finding the best combination of kernel hyperparameters\n",
    "\n",
    "We can do this by maximizing the marginal likelihood. For regression with iid Gaussian noise the marginal likelihood $y$ is also Gaussian\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}) = \\int p(\\textbf{y}|f) p(f) \\,df = \\mathcal{N}(0, K + \\sigma_\\epsilon^2 I )\n",
    "$$\n",
    "\n",
    "where the hyperparameter $\\sigma_\\epsilon^2$ is the variance of the noise \n",
    "\n",
    "It is equivalent and much easier to maximize the log marginal likelihood\n",
    "\n",
    "$$\n",
    "\\log p(\\textbf{y}|\\textbf{x}) = -\\frac{1}{2} \\textbf{y}^T (K + \\sigma_\\epsilon^2I)^{-1} \\textbf{y} - \\frac{1}{2} \\log | 2\\pi (K + \\sigma_\\epsilon^2I) |\n",
    "$$\n",
    "\n",
    "from which we compute derivatives to update the hyperparameters through gradient descent\n",
    "\n",
    "The following picture from Barber's book shows three examples drawn from the GP prior (gaussian kernel) on the left and the mean/variance of the GP posterior after training on the right\n",
    "\n",
    "<img src=\"images/gp_fitted.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are GP and NN related?\n",
    "\n",
    "[(Neil 1994)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf) showed that a fully connected neural network with one hidden layer tends to a Gaussian process in the limit of infinite hidden units as a consequence of the central limit theorem. \n",
    "\n",
    "- [Sumary of the proof](https://pillowlab.wordpress.com/2019/04/14/deep-neural-networks-as-gaussian-processes/)\n",
    "- [Deep Neural Networks as Gaussian Processes](https://openreview.net/forum?id=B1EA-M-0Z)\n",
    "- [Gaussian Process Behaviour in Wide Deep Neural Networks](https://arxiv.org/abs/1804.11271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes with `pyro`\n",
    "\n",
    "### Setting, training and performing inference \n",
    "\n",
    "Let's start by creating some synthetic data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) #100x1\n",
    "x_test = np.linspace(-0.4, 1.6, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [`pyro.contrib.gp`](http://docs.pyro.ai/en/stable/contrib.gp.html) to implement our first GP\n",
    "\n",
    "Let's start by creating a kernel from `gp.kernels`\n",
    "\n",
    "We will use a Radial Basis Function (RBF) aka Squared Exponential aka Gaussian kernel as our covariance\n",
    "\n",
    "We can specify the initial value of the variance and the lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.), \n",
    "                   lengthscale=torch.tensor(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model looks before fitting the data? \n",
    "\n",
    "Let's inspect the prior $p(f) = \\mathcal{N}(0, K)$ on the test data\n",
    "\n",
    "**Activity:** Increase/decrese the lengthscale and repeat, get a notion of its influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "C = K.forward(torch.from_numpy(x_test)) + torch.eye(len(x_test))*1e-4\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(torch.zeros(len(x_test)), \n",
    "                                                covariance_matrix=C).sample(sample_shape=(20,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :],\n",
    "            linestyle='-', c='tab:blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train GP and plot the results\n",
    "\n",
    "def train_gp_plots(model, x, y, x_test, nepochs=2000):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "    line_loss = ax[1].plot([], [])\n",
    "    ax[0].scatter(x, y)\n",
    "    epoch_loss = np.zeros(shape=(nepochs,))\n",
    "\n",
    "    for k in tqdm(range(len(epoch_loss))):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model.model, model.guide)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss[k] = loss.item()\n",
    "        #break    \n",
    "        if k % 100 == 0:\n",
    "            ax[0].cla()\n",
    "            # Predictions at x_test\n",
    "            mu, cov = model.forward(x_test, full_cov=True, noiseless=False)\n",
    "            mu = mu.detach().numpy()\n",
    "            sd = cov.diag().sqrt().detach().numpy()        \n",
    "            ax[0].scatter(x, y, c='k')\n",
    "            ax[0].plot(x_test.detach(), mu)\n",
    "            ax[0].fill_between(x_test.detach(), mu-2*sd, mu+2*sd, alpha=0.5)\n",
    "            line_loss[0].set_xdata(range(k))\n",
    "            line_loss[0].set_ydata(epoch_loss[:k])\n",
    "            ax[1].relim()\n",
    "            ax[1].autoscale_view()\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model from `gp.models`. For regression there is `GPRegression` and for classification (non-gaussian likelihoods) we can use `VariationalGP`. There are also more efficient (sparse) versions of both models\n",
    "\n",
    "The model expects \n",
    "\n",
    "- the train data\n",
    "- the kernel \n",
    "- initial value of the noise variance\n",
    "\n",
    "We may also specify a mean function for the GP\n",
    "\n",
    "To train the model we have to select an optimizer and a cost function. We will use Adam and the Trace_ELBO, respectively\n",
    "\n",
    "Training is very similar to how we train neural networks in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.01))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, # Training data\n",
    "                                   mean_function=None, # Mean\n",
    "                                   kernel=K, # Covarianze\n",
    "                                   jitter=1e-6, # Increase this if you have numerical problems \n",
    "                                   noise=torch.tensor(2.) # The variance of the white noise\n",
    "                                   )\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"RBF variance:\", gpr_model.kernel.variance.item())\n",
    "display(\"RBF length scale:\", gpr_model.kernel.lengthscale.item())\n",
    "display(\"Noise variance:\", gpr_model.noise.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do predictions we use the forward attribute of our `GPRegression` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "mu, Sigma = gpr_model.forward(torch.from_numpy(x_test), full_cov=True, noiseless=True)\n",
    "Sigma += torch.eye(len(x_test))*1e-5\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(mu, covariance_matrix=Sigma).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :], \n",
    "            linestyle='-', c='tab:blue', alpha=0.25)\n",
    "ax.scatter(x, y, c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different kernels\n",
    "\n",
    "Kernel are implemented in [pyro.contrib.gp.kernels](http://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.kernels)\n",
    "\n",
    "Compare the RBF and Matern52 kernels. What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "#K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "K = gp.kernels.Matern52(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "#K = gp.kernels.Periodic(input_dim=1, variance=torch.tensor(2.0), \n",
    "#                        lengthscale=torch.tensor(0.1), \n",
    "#                        period=torch.tensor(2.))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting priors for the parameters \n",
    "\n",
    "Before we did an MLE-like estimation to find point estimates of the kernel hyper-parameters and the noise variance\n",
    "\n",
    "We can \"go bayesian\" and treat these parameters as random variables and set priors for them\n",
    "\n",
    "Training with these priors is equivalent to the MAP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "from pyro.distributions import LogNormal\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model_prior = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "\n",
    "# Setting priors\n",
    "gpr_model_prior.kernel.lengthscale = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "gpr_model_prior.kernel.variance = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model_prior.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot    \n",
    "train_gp_plots(gpr_model_prior, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining kernels\n",
    "\n",
    "Adding or multiplying valid kernels yield a valid kernel\n",
    "\n",
    "> We can easily create new kernels from other kernels\n",
    "\n",
    "and take advantage of their different properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data has a periodic oscilation on a rising trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100).astype('float32')*100\n",
    "y = (0.03*x + np.sin(0.1*x) + 0.1*np.random.randn(100)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fit it using `K1`, `K2`, `Ksum12` and `Kprod12`\n",
    "\n",
    "Can you explain in simple words your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "K1 = gp.kernels.Periodic(input_dim=1, variance=torch.tensor(1.), \n",
    "                        lengthscale=torch.tensor(10),\n",
    "                        period=torch.tensor(50))\n",
    "K2 = gp.kernels.Linear(input_dim=1, variance=torch.tensor(1.))\n",
    "Ksum12 = gp.kernels.Sum(K1, K2)\n",
    "Kprod12 = gp.kernels.Product(K1, K2)\n",
    "\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(torch.from_numpy(x), torch.from_numpy(y), \n",
    "                                   kernel=K1, noise=torch.tensor(2.))\n",
    "\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "train_gp_plots(gpr_model, x, y, torch.linspace(-50, 150, 100), nepochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Gaussian Processes with `pyro`\n",
    "\n",
    "Fitting a Gaussian process has cubic complexity\n",
    "\n",
    "Sparse Gaussian processes use a much smaller set of \"inducing points\" to compute the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=1000) #100x1\n",
    "x_test = np.linspace(-0.1, 1.1, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "#gpr_model = gp.models.SparseGPRegression(x_torch, y_torch, approx='VFE',\n",
    "#                                         kernel=K, Xu=torch.linspace(0, 1, 20),\n",
    "#                                         noise=torch.tensor(2.), jitter=1e-5)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics for the future\n",
    "\n",
    "- Example of GP for classification (Bernoulli likelihood), Barber 19.5\n",
    "- Deep Gaussian Processes/Stacks of Gaussian Processes. Example in Pyro: https://fehiepsi.github.io/blog/deep-gaussian-process/\n",
    "- [Compositional kernel learning](https://arxiv.org/pdf/1806.04326.pdf)\n",
    "- Bayesian optimization with Gaussian processes\n",
    "- https://github.com/team-approx-bayes/dnn2gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study\n",
    "\n",
    "- Mackay's book, chapter 45 on Gaussian Proceses\n",
    "- Barber's book, chapter 19 on Gaussian Processes\n",
    "- [Rasmussen & Willams, \"Gaussian Process for Machine Learning\"](http://gaussianprocess.org/gpml/?)\n",
    "- [Zhoubin Ghahramadi tutorial](http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
