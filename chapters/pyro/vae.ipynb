{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pyro\n",
    "display(pyro.__version__)\n",
    "\n",
    "# Data for this lecture\n",
    "import torchvision\n",
    "mnist_data = torchvision.datasets.MNIST('~/datasets', train=True, download=True,\n",
    "                                        transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(6, 1), tight_layout=True)\n",
    "for i in range(10):\n",
    "    image, label = mnist_data[i]\n",
    "    ax[i].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(label)\n",
    "    \n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = list(range(len(mnist_data)))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.8*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_data, batch_size=128, drop_last=False,\n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_data, batch_size=128, drop_last=False,\n",
    "                          sampler=SubsetRandomSampler(idx[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models (LVM)\n",
    "\n",
    "## Intuition: Modeling with latent variables\n",
    "\n",
    "Let's say we have a dataset $X = \\{ \\textbf{x}_1, \\textbf{x}_2, \\ldots, \\textbf{x}_N\\}$ with $\\dim (\\textbf{x}) =D$ and we want to model the generative distribution $p(\\textbf{x})$\n",
    "\n",
    "Each sample has $D$ components or attributes (e.g. the pixels of an image): These are the **observed variables** \n",
    "\n",
    "To model $p(\\textbf{x})$ we may expand the joint between the attributes using the rules of probability\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, \\ldots, x_D) = p(x_D|x_{D-1}, \\ldots, x_1) \\cdot p(x_{D-1}| , x_{D-1}\\ldots, x_1) \\cdots p(x_3|x_1, x_1) \\cdot p(x_2|x_1) \\cdot p(x_1)\n",
    "$$\n",
    "\n",
    "which is known as a **fully observed model**. Unless we introduce independence between some of the variables the above representation is impractical for high dimensional problems (e.g. images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One alternative is to assume that \n",
    "\n",
    "> what we observe is correlated due to *hidden causes*\n",
    "\n",
    "These hidden causes are represented as **latent variables** and models with latent variables are called **Latent Variable Models** (LVMs)\n",
    "\n",
    "Mathematically, we impose that the observed variables are conditionally independent given the latent variables $\\textbf{z}$, this is\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, \\ldots, x_D|\\textbf{z}) = p(x_D|\\textbf{z}) \\cdot p(x_{D-1}|\\textbf{z}) \\cdots p(x_3|\\textbf{z}) \\cdot p(x_2|\\textbf{z}) \\cdot p(x_1|\\textbf{z})\n",
    "$$\n",
    "\n",
    "where in general $\\dim(\\textbf{z})\\ll\\dim(\\textbf{x})$\n",
    "\n",
    "The following figure shows the graphical model of a fully observed model with five observed variables and an LVM with two latent variables\n",
    "\n",
    "<img src=\"images/LVM.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LVM we can write the marginal as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\textbf{x}) &= \\int_z p(\\textbf{x}, \\textbf{z}) \\,d\\textbf{z} \\nonumber \\\\\n",
    "&= \\int_\\textbf{z} p(\\textbf{x}|\\textbf{z}) p(\\textbf{z}) \\,d\\textbf{z} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Did we gain anything?  YES\n",
    "\n",
    "> This strategy allows us to model a complex $p(x)$ by proposing a simple $p(z)$ (easy to sample from) and a transformation $p(x|z)$ \n",
    "\n",
    "The integral above is intractable for non-linear transformations (neural networks), in that case we resort to approximate inference\n",
    "\n",
    "This lecture is focused on LVMs for continuous data. First we will review an example with a tractable posterior (PCA) and then the more modern LVM based on neural networks: The Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review of PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is an algorithm to reduce the dimensionality of continous data\n",
    "\n",
    "For a dataset $X = (x_1, x_2, \\ldots, x_N) \\in \\mathbb{R}^{N \\times D}$, in PCA we \n",
    "\n",
    "1. Compute covariance matrix $C = \\frac{1}{N} X^T X$\n",
    "1. Solve the eigenvalue problem $(C - \\lambda I)W = 0$\n",
    "\n",
    "This comes from the following objective\n",
    "\n",
    "$$\n",
    "\\min_W W^T C W, \\text{s.t.} ~ W^T W = I,\n",
    "$$\n",
    "\n",
    "i.e. PCA finds an **orthogonal transformation** $W$ that **minimizes the variance** of the projected data $XW$\n",
    "\n",
    "By reducing the amount of columns of $W$ we reduce the dimensionality of $XW$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Classical PCA for MNIST using PyTorch\n",
    "\n",
    "Implementation of PCA using [`symeig`](https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, data, K=2):\n",
    "        self.data_mean = torch.mean(data, dim=0)\n",
    "        data_centered = data - self.data_mean.expand_as(data)\n",
    "        C = torch.matmul(data_centered.T, data_centered)\n",
    "        # V is sorted in increasing order\n",
    "        V, W = torch.symeig(C, eigenvectors=True)\n",
    "        self.W = W[:, -K:]\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return torch.mm(x - self.data_mean.expand_as(x), self.W)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.data_mean + torch.mm(z, self.W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the $28\\times28$ observed dimensions of PCA are projected to two continuous latent variables \n",
    "\n",
    "We can then inspect the latent space and images reconstructed from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = mnist_data.data.reshape(-1, 28*28)/255.\n",
    "pca = PCA(images, K=2)\n",
    "Z = pca.encode(images)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "for digit in range(10):\n",
    "    mask = mnist_data.targets == digit\n",
    "    ax.scatter(Z[mask, 0].detach().numpy(), Z[mask, 1].detach().numpy(), \n",
    "               s=5, alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "plt.legend()\n",
    "ax.set_xlabel('PC 1'); ax.set_ylabel('PC 2');\n",
    "ax.set_title('Latent space')\n",
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "reconstructions = pca.decode(Z[:10, :]).reshape(-1, 28, 28).detach().numpy()\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(mnist_data.data[i, :].reshape(28, 28).detach().numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(reconstructions[i], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')\n",
    "ax[0, 0].set_ylabel('Original')\n",
    "ax[1, 0].set_ylabel('Reconstructed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important principal components in this case are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(4, 1.5), tight_layout=True)\n",
    "for i in range(2):\n",
    "    ax[i].imshow(pca.W[:, i].reshape(28, 28).detach().numpy())\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title('PC %d' %(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two continous latent variables are not enough to model the digits given this linear model. Later we will see how this changes using non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation for PCA\n",
    "\n",
    "We can give a probabilistic interpretation to PCA as an LVM\n",
    "\n",
    "We start by modeling an observed sample $x_i \\in \\mathbb{R}^D$ as\n",
    "\n",
    "$$\n",
    "x_i = W z_i + B + \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "- $B \\in \\mathbb{R}^D$ is the mean of $X$\n",
    "- $W \\in \\mathbb{R}^{D\\times K}$ is a linear transformation matrix\n",
    "- $\\epsilon$ is the noise\n",
    "- $z_i \\in  \\mathbb{R}^K$ is a continuous latent variable with $K\\ll D$\n",
    "- $x$ (observed) is related to $z$ (latent) via a **linear mapping**\n",
    "\n",
    "This model has the following assumptions\n",
    "\n",
    "1. The noise is independent and Gaussian distributed with variance $\\sigma^2$\n",
    "1. The latent variable has a standard Gaussian prior\n",
    "\n",
    "Using these we can write\n",
    "\n",
    "$$\n",
    "p(x_i | z_i) = \\mathcal{N}(B + W z_i, I \\sigma^2)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Given that the Gaussian is conjugate to itself the marginal likelihood is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int p(x|z) p(z) \\,dz \\nonumber \\\\\n",
    "&= \\mathcal{N}(x|B, W W^T + I\\sigma^2 ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that we have parameterized a Gaussian with full covariance from to Gaussians with diagonal covariance\n",
    "\n",
    "The parameters of the marginal come from\n",
    "\n",
    "- $\\mathbb{E}[x] = W\\mathbb{E}[z] + B + \\mathbb{E}[\\epsilon] = B$\n",
    "- $\\mathbb{E}[(Wz + \\epsilon)(Wz + \\epsilon)^T] = W \\mathbb{E}[zz^T] W^T + \\mathbb{E}[\\epsilon \\epsilon^T] = W W^T + I\\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The posterior**\n",
    "\n",
    "Using this formalism we can write the posterior to go from observed to latent as\n",
    "\n",
    "$$\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-B), M\\sigma^{-2} )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M = W^T W + I\\sigma^2\n",
    "$$\n",
    "\n",
    "**Training**\n",
    "\n",
    "We find $W$, $B$ and $\\sigma$ that best fit the data by maximizing the log marginal likelihood\n",
    "\n",
    "$$\n",
    "\\hat W, \\hat B, \\hat \\sigma^2 = \\text{arg} \\max_{W, B, \\sigma^2} \\sum_{i=1}^N \\log p(x_i)\n",
    "$$\n",
    "\n",
    "which has a closed form analytical solution. Note that the solution for $W$ is equivalent to conventional PCA ($\\sigma^2 \\to 0$). The main difference is that we have $\\sigma$ and we can generate data with $p(x|z)p(z)$\n",
    "\n",
    "\n",
    "For more details on the probabilistic PCA see chapter 21 and Murphy, Chapter 12\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review of autoencoders\n",
    "\n",
    "An autoencoder is an artificial neural networks for representation learning and dimensionality reduction\n",
    "\n",
    "The schematic exemplifies the architecture of an autoencoder \n",
    "\n",
    "<img src=\"images/ae.png\" width=\"800\">\n",
    "\n",
    "In general\n",
    "\n",
    "- The input and output dimensionality are equivalent\n",
    "- The code or bottleneck has a smaller dimensionality than the input/output\n",
    "\n",
    "We call **encoder** to the neural net that maps the input to the code\n",
    "\n",
    "$$\n",
    "z = g_\\phi(x)\n",
    "$$\n",
    "\n",
    "and **decoder** to the neural net that maps the code to the output\n",
    "\n",
    "$$\n",
    "\\hat x = f_\\theta(z)\n",
    "$$\n",
    "\n",
    "Autoencoders are trained by minimzing an error, e.g. the mean square error (MSE) or cross-entropy, between the input and the output, i.e. the data is used as target (self-supervision)\n",
    "\n",
    "For example we may use the MSE\n",
    "\n",
    "$$\n",
    "\\hat \\theta, \\hat \\phi = \\text{arg} \\min_{\\phi, \\theta} \\| x - f_\\theta(g_\\phi(x)) \\|^2\n",
    "$$\n",
    "\n",
    "which is equivalent to the maximum likelihood (MLE) solution assuming a spherical Gaussian likelihood function (cross entropy is equivalent to the MLE given a Bernoulli likelihood)\n",
    "\n",
    "Adding an L2 regularizer on $\\theta$ and $\\phi$ is equivalent to incorporating a spherical gaussian prior (MAP solution)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Autoencoder for MNIST in pytorch\n",
    "\n",
    "In this example we define one module for the encoder and one for the decoder, each with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.output(h)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.code = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return (self.code(h))\n",
    "    \n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=28*28, hidden_dim=128):\n",
    "        super(AutoEncoder, self).__init__() \n",
    "        self.encoder = Encoder(latent_dim, input_dim, hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim, hidden_dim=hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we use the binary cross entropy and the ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(latent_dim=2)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "for nepoch in tqdm(range(50)):\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        hatx = model.forward(x.reshape(-1, 28*28))\n",
    "        loss = criterion(hatx, x.reshape(-1, 28*28))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"{nepoch}: {epoch_loss/(len(train_loader)*train_loader.batch_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space and reconstructions in this case are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "Y = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "\n",
    "for x, y in train_loader:\n",
    "    Z = torch.cat((Z, model.encoder(x.reshape(-1, 28*28))))\n",
    "    Y = torch.cat((Y, y))\n",
    "\n",
    "Z = Z.detach().cpu().numpy()\n",
    "Y = Y.detach().cpu().numpy()                  \n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "for digit in range(10):\n",
    "    mask = Y == digit\n",
    "    ax.scatter(Z[mask, 0], Z[mask, 1], \n",
    "               s=5, alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "plt.legend()\n",
    "\n",
    "output_activation = torch.nn.Sigmoid()\n",
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "hatx = model.forward(x.reshape(-1, 28*28))\n",
    "reconstructions = output_activation(hatx).reshape(-1, 28, 28).detach().cpu().numpy()\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(x.detach().cpu().numpy()[i+10, 0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(reconstructions[i+10], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference  for LVM\n",
    "\n",
    "The LVM is defined by the joint density between observed and latent variables\n",
    "\n",
    "$$\n",
    "p(\\textbf{x}, \\textbf{z}) = \\prod_{i=1}^N p(\\textbf{x}_i|\\textbf{z}_i) p(\\textbf{z}_i)\n",
    "$$\n",
    "\n",
    "If we use the **PCA recipe** (Linear mapping, Gaussian likelihood and Gaussian prior) we obtain an analytical Gaussian posterior and evidence\n",
    "\n",
    "If we use a more complex (non-linear) mapping the posterior and evidence may not be tractable. In such case, we can use **Variational inference (VI)**, i.e. we propose an approximate posterior and maximize the ELBO\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x) \\geq \\mathcal{L}(\\phi) &= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left[\\log \\frac{p(x, z)}{q_\\phi(z|x)}\\right] \\nonumber \\\\\n",
    "&= \\int q_\\phi(z|x) \\log \\frac{p(x, z)}{q_\\phi(z|x)} dz \\nonumber \n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "to find the best parameters $\\hat \\phi$. \n",
    "\n",
    "In what follows we review the variational autoencoder which combines amortized inference and VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "The Variational Autoencoder (VAE) is an LVM where **deep neural networks** are used to model the **conditional distributions** between latent $z$ and observed $x$ variables\n",
    "\n",
    "It was proposed simultaneously by [(Kingma and Welling, ICLR, Dec. 2013)](https://arxiv.org/pdf/1312.6114.pdf) and [(Rezende *et al*, ICML, Jan. 2014)](https://arxiv.org/abs/1401.4082) perhaps sparking the revived interest into **Deep Learning plus Approximate Bayesian Inference** that we see today\n",
    "\n",
    "\n",
    "The difference with a regular autoencoder is that the latent (code) is now a stochastic variable\n",
    "\n",
    "- a prior distribution is placed on $z$: $p(z)$\n",
    "- a neural network is used to model the parameters of the likelihood: $p_\\theta(x | z)$\n",
    "- a neural network is used to model the parameters of the approximate posterior: $q_\\phi(z|x)$\n",
    "- The weight and biases of the networks $\\theta$ and $\\phi$ are deterministic, *i.e.* VAE is not a \"fully\" bayesian neural network\n",
    "\n",
    "In VAE, variational inference is used to obtain the posterior and point estimates of the global parameters\n",
    "\n",
    "In what follows we will review the assumptions and the key contributions of this work to the field of Bayesian Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "VAE assumes a particular prior and approximate posterior. The likelihood function is chosen depending on the data\n",
    "\n",
    "1. The latent variable has a standard Gaussian prior (the same as in PCA)\n",
    "1. The approximate posterior is a factorized (diagonal) Gaussian\n",
    "1. For continuous data the likelihood is typically set as a spherical Gaussian or diagonal Gaussian. For binary data the likelihood is set as Bernoulli. \n",
    "\n",
    "Mathematically this is \n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q(z_i|x_i) = \\mathcal{N}(\\mu_i, I \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "p(x_i|z_i) = \\mathcal{N}(\\hat \\mu_i, I \\hat \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "for $x_i \\in \\mathbb{R}^D$. \n",
    "\n",
    "### Amortization\n",
    "\n",
    "In the previous formulation the amount of variational parameters ($\\mu_i$ and $\\sigma_i$) scales linearly with $N$. This is impractical for large datasets. \n",
    "\n",
    "Instead of having parameters per data point we can have a function that maps the data to the parameters. This is known as **amortization**\n",
    "\n",
    "In the particular case of VAE we have\n",
    "\n",
    "$$\n",
    "\\mu_i, \\sigma_i = g_\\phi(x_i)\n",
    "$$\n",
    "\n",
    "where $g_\\phi(\\cdot)$ is the **encoder network** and\n",
    "\n",
    "$$\n",
    "\\hat \\mu_i, \\hat \\sigma_i = f_\\theta(z_i)\n",
    "$$\n",
    "\n",
    "where $f_\\theta(\\cdot)$ is the **decoder network** (for a diagonal Gaussian likelihood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on the VAE training\n",
    "\n",
    "The VAE is trained by maximizing the ELBO, which in this case is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi) &= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) + \\log p(z) - \\log q_\\phi(z|x) \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) \\right ] - D_{KL}\\left[ q_\\phi(z|x) || p(z) \\right]\\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By maximizing the ELBO we\n",
    "\n",
    "- Maximize the log likelihood when sampling from the approximate posterior: **Faithfull data reconstructions**\n",
    "- Minimize the divergence between the approximate posterior and prior: **Regularization for the posterior**\n",
    "\n",
    "\n",
    "The ELBO is typically optimized via gradient ascent updates for $\\theta$ and $\\phi$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} + \\eta \\nabla_\\theta \\mathcal{L}(\\theta_{t}, \\phi_{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_{t+1} = \\phi_{t} + \\eta \\nabla_\\phi \\mathcal{L}(\\theta_{t}, \\phi_{t})\n",
    "$$\n",
    "\n",
    "In what follows we review how to obtain the derivates of the ELBO with respect to $\\theta$ and $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The derivative with respect to $\\theta$** \n",
    "\n",
    "The only term that depends on $\\theta$ is $p_\\theta(x|z)$, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta, \\phi)  &= \\nabla_\\theta \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [\\log p_\\theta(x|z)\\right ] \\nonumber\\\\ &= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\nabla_\\theta \\log  p_\\theta(x|z)\\right ] \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the gradient can be swapped with the expectation operator. The expectation can be estimated via monte-carlo integration as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\nabla_\\theta \\log  p_\\theta(x|z)\\right ] &= \\int q_\\phi(z|x) \\nabla_\\theta \\log  p_\\theta(x|z) \\,dz \\nonumber \\\\&\\approx \\frac{1}{S} \\sum_{k=1}^S  \\nabla_\\theta \\log p_\\theta(x|z^{(k)}) \\quad z^{(k)} \\sim  q_\\phi(z|x) \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The derivative wrt to $\\phi$**\n",
    "\n",
    "Let's write $f(z) = \\log p_\\theta(x|z)$, a function of $z$ that does not depend on $\\phi$. In this case\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi  \\mathcal{L}(\\theta, \\phi) &=  \\nabla_\\phi \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z) \\right ] \\nonumber \\\\ &= \\nabla_\\phi \\int q_\\phi(z|x) f(z) dz \\nonumber \\\\ &=  \\int f(z) \\nabla_\\phi q_\\phi(z|x)  dz \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but we cannot approximate via monte carlo integration anymore. \n",
    "\n",
    "A classical solution to this is the [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) algorithm, which is based on the identity \n",
    "\n",
    "$$\n",
    "\\nabla_\\phi\\log q_\\phi(z) = \\frac{1}{ q_\\phi(z)} \\nabla_\\phi q_\\phi(z) \n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z)\\right ]  &= \\int f(z) \\nabla_\\phi q_\\phi(z|x)  dz \\nonumber \\\\ &= \\int q_\\phi(z) f(z) \\nabla_\\phi\\log q_\\phi(z)  dz \\nonumber \\\\ &\\approx \\frac{1}{S} \\sum_{k=1}^S f(z^{(k)}) \\nabla_\\phi \\log q_\\phi(z^{(k)}|x) \\quad z^{(k)} \\sim  q_\\phi(z|x) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but in practice training with this estimator is very hard. Although the REINFORCE estimator is unbiased it has a very high variance\n",
    "\n",
    "For the particular case of VAE there is a low variance (and very elegant) alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key contribution**: The reparameterization trick\n",
    "\n",
    "In VAE the latent variable is distributed as\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2 (x) )\n",
    "$$\n",
    "\n",
    "we can rewrite this as first sampling from a standard Gaussian \n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "and then applying a transformation\n",
    "\n",
    "$$\n",
    "z = g(\\phi, \\epsilon) = \\mu_\\phi (x) + \\epsilon \\sigma_\\phi (x)  \n",
    "$$\n",
    "\n",
    "Then we can rewrite the expectation of $f(z)$ as \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z) \\right ] =  \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f(g(\\phi, \\epsilon))  \\right ] \n",
    "$$\n",
    "\n",
    "Now that the expectation does not depend on $\\phi$ we can use the following estimator for its gradient\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f(g(\\phi, \\epsilon))  \\right ] &= \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f'(g(\\phi, \\epsilon)) \\nabla_\\phi g(\\phi, \\epsilon) \\right ] \\nonumber \\\\\n",
    "&\\approx \\frac{1}{S} \\sum_{k=1}^S f'(g(\\phi, \\epsilon^{(k)})) \\nabla_\\phi g(\\phi, \\epsilon^{(k)}) \\quad \\epsilon^{(k)} \\sim  \\mathcal{N}(0,I) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which has a [much lower variance than REINFORCE](https://nbviewer.jupyter.org/github/gokererdogan/Notebooks/blob/master/Reparameterization%20Trick.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More variance reduction:** Closed-form terms\n",
    "\n",
    "We have focused on the left hand term of the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) \\right ] - D_{KL}\\left[ q_\\phi(z|x) || p(z) \\right]\n",
    "$$\n",
    "\n",
    "The right hand term is the KL divergence between two multivariate Gaussian distributions. This has a [closed analytical solution](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)\n",
    "\n",
    "$$\n",
    "D_\\text{KL}\\left[q_\\phi(z|x) || p(z) \\right] = \\frac{1}{2}\\sum_{j=1}^K \\left(\\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right)\n",
    "$$\n",
    "\n",
    "where $K$ is the dimensionality of the latent variable\n",
    "\n",
    "The derivatives of this estimater are straighforward and the variance is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`pyro` related notes**\n",
    "\n",
    "When [`TraceMeanField_ELBO`](https://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.trace_mean_field_elbo.TraceMeanField_ELBO) is used pyro assumes that the latent variables in the guide can be reparameterized and uses the analytical KL\n",
    "\n",
    "You can read more on variance reduction in this official [pyro tutorial](https://pyro.ai/examples/svi_part_iii.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a VAE in Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous AE example we use two hidden layers, but now the encoder is \"dual-headed\", i.e. it outputs the parameters of the factorized gaussian associated to the latent variable\n",
    "\n",
    "We have to make sure that the standard deviation is non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDual(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=28*28, hidden_dim=128):\n",
    "        super(EncoderDual, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.z_loc = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_scale = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.z_loc(h), torch.exp(self.z_scale(h))\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim=28*28, hidden_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.output(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the model and the guide within a `torch.nn.Module` \n",
    "\n",
    "To register the weights and biases of `Encoder` and `Decoder` to the parameter dictionary of pyro we register them using the [`pyro.module`](https://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.module) primitive, i.e. we don't need to use `pyro.param` in this case. \n",
    "\n",
    "We use plates to make the model conditionally independent on the batch dimension (leftmost dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dists\n",
    "\n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, data_dim=28*28, hidden_dim=128):\n",
    "        super(VariationalAutoEncoder, self).__init__() \n",
    "        self.encoder = EncoderDual(latent_dim, input_dim=data_dim, hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_dim=data_dim, hidden_dim=hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # p(z)\n",
    "            z_loc = torch.zeros(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z_scale = torch.ones(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z = pyro.sample(\"latent\", dists.Normal(z_loc, z_scale).to_event(1))\n",
    "            # p(x|z)\n",
    "            p_logits = self.decoder.forward(z)\n",
    "            pyro.sample(\"observed\", \n",
    "                        dists.Bernoulli(logits=p_logits, validate_args=False).to_event(1), \n",
    "                        obs=x.reshape(-1, 28*28))\n",
    "    \n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # q(z|x)\n",
    "            z_loc, z_scale  = self.encoder.forward(x.reshape(-1, 28*28))\n",
    "            pyro.sample(\"latent\", \n",
    "                        dists.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using SVI with the ClippedAdam optimizer\n",
    "\n",
    "The guide satisfies the mean field condition, this means we can use `TraceMeanField_ELBO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) \n",
    "pyro.clear_param_store()\n",
    "\n",
    "vae = VariationalAutoEncoder(latent_dim=2)\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    vae = vae.cuda()\n",
    "    \n",
    "svi = pyro.infer.SVI(model=vae.model, \n",
    "                     guide=vae.guide, \n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 1e-2}), \n",
    "                     loss=pyro.infer.TraceMeanField_ELBO(num_particles=5, \n",
    "                                                         vectorize_particles=True))\n",
    "\n",
    "for nepoch in tqdm(range(50)):    \n",
    "    epoch_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        epoch_loss += svi.step(x)\n",
    "    print(f\"{nepoch}: {epoch_loss/(len(train_loader)*train_loader.batch_size):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the latent space, errorbars are used to show the mean and variance of the latent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    vae = vae.cpu()\n",
    "    \n",
    "Z = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "Y = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "\n",
    "for x, y in valid_loader:\n",
    "    Z = torch.cat((Z, torch.cat(vae.encoder(x.reshape(-1, 28*28)), dim=1)), dim=0)\n",
    "    Y = torch.cat((Y, y))\n",
    "\n",
    "Z = Z.detach().cpu().numpy()\n",
    "Y = Y.detach().cpu().numpy()                  \n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "for digit in range(10):\n",
    "    mask = Y == digit\n",
    "    ax.errorbar(x=Z[mask, 0], y=Z[mask, 1], \n",
    "                xerr=Z[mask, 2], yerr=Z[mask, 3], \n",
    "                fmt='none', alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows reconstructions as a function of $z$\n",
    "\n",
    "The white contour represents the approximate posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 30\n",
    "z_plot = np.linspace(-3, 3, num=M)\n",
    "big_imag = np.zeros(shape=(28*M, 28*M))\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        z = torch.tensor(np.array([z_plot[j], z_plot[M-1-i]]), dtype=torch.float32)\n",
    "        xhat = output_activation(vae.decoder.forward(z)).reshape(28, 28). detach().numpy()\n",
    "        big_imag[i*28:(i+1)*28, j*28:(j+1)*28] = xhat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), tight_layout=True)\n",
    "Z_plot1, Z_plot2 = np.meshgrid(z_plot, z_plot)\n",
    "ax.matshow(big_imag, vmin=0.0, vmax=1.0, cmap=plt.cm.gray, extent=[-4, 4, -4, 4])\n",
    "H, xedge, yedge = np.histogram2d(Z[:, 0], Z[:, 1], bins=30, range=[[-4, 4], [-4, 4]])\n",
    "ax.contour(Z_plot1, Z_plot2, H.T, linewidths=3, levels=[1], cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
