{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models (LVM)\n",
    "\n",
    "\n",
    "Let's say we want to model a dataset $X = (x_1, x_2, \\ldots, x_N)$ with $x_i \\in \\mathbb{R}^D$ \n",
    "\n",
    "> We are looking for $p(x)$\n",
    "\n",
    "Each sample has D attributes\n",
    "\n",
    "> These are the **observed variables** (visible space)\n",
    "\n",
    "To model the data we have to propose dependency relationships between variables\n",
    "\n",
    "> Modeling correlation is difficult\n",
    "\n",
    "One alternative is to assume that what we observe is correlated due to *hidden causes*\n",
    "\n",
    "> These are the **latent variables** (hidden space)\n",
    "\n",
    "Models with latent variables are called **Latent Variable Models** (LVM)\n",
    "\n",
    "Then we get the marginal using\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int_z p(x, z) \\,dz \\nonumber \\\\\n",
    "&= \\int_z p(x|z) p(z) \\,dz \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Did we gain anything? \n",
    "\n",
    "> The integral can be hard to solve (in some cases it is tractable)\n",
    "\n",
    "The answer is YES\n",
    "\n",
    "> We can propose simple $p(x|z)$ and $p(z)$ and get complex $p(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "## Classical PCA\n",
    "\n",
    "PCA is an algorithm to reduce the dimensionality of continous data\n",
    "\n",
    "Let's say we have $X = (x_1, x_2, \\ldots, x_N)$ con $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "In classical PCA we \n",
    "\n",
    "1. Compute covariance matrix $C = \\frac{1}{N} X^T X$\n",
    "1. Solve the eigen value problem $(C - \\lambda I)W = 0$\n",
    "\n",
    "This comes from \n",
    "\n",
    "$$\n",
    "\\min_W W^T C W, \\text{s.t.} ~ W^T W = I\n",
    "$$\n",
    "\n",
    "> PCA finds an **orthogonal transformation** $W$ that **minimizes the variance** of the projected data $XW$\n",
    "\n",
    "Then we can reduce the amount of columns of $W$ to reduce the dimensionality of $XW$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_train_data = torchvision.datasets.MNIST('dataset', train=True, download=True,\n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "mnist_test_data = torchvision.datasets.MNIST('dataset', train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "np.random.seed(0)\n",
    "#idx = list(range(len(mnist_train_data)))\n",
    "idx = list(range(10000))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.7*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, batch_size=128, \n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_train_data, batch_size=256, \n",
    "                          sampler=SubsetRandomSampler(idx[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation\n",
    "\n",
    "We can give a probabilistic interpretation to PCA as an LVM\n",
    "\n",
    "An observed sample $x_i \\in \\mathbb{R}^D$ is modeled as \n",
    "\n",
    "$$\n",
    "x_i = W z_i + B + \\epsilon\n",
    "$$\n",
    "\n",
    "> Observed variable is related to the latent variable via a **linear mapping**\n",
    "\n",
    "where \n",
    "- $B \\in \\mathbb{R}^D$ is the mean of $X$\n",
    "- $W \\in \\mathbb{R}^{D\\times K}$ is a linear transformation matrix\n",
    "- $\\epsilon$ is noise\n",
    "\n",
    "> $z_i \\in  \\mathbb{R}^K$ is a continuous latent variable with $K<D$\n",
    "\n",
    "#### Assumption: The noise is independent and Gaussian distributed with variance $\\sigma^2$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "p(x_i | z_i) = \\mathcal{N}(B + W z_i, I \\sigma^2)\n",
    "$$\n",
    "\n",
    "Note: In general factor analysis the noise has a diagonal covariance\n",
    "\n",
    "#### Assumption: The latent variable has a standard Gaussian prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Marginal likelihood\n",
    "\n",
    "The Gaussian is conjugated to itself (convolution of Gaussians is Gaussian)\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int p(x|z) p(z) \\,dz \\nonumber \\\\\n",
    "&= \\mathcal{N}(x|B, W^T W + I\\sigma^2 ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> We have parametrized a normal with full covariance from to normals with diagonal covariance\"\n",
    "\n",
    "The parameters are calculated from \n",
    "- $\\mathbb{E}[x] = W\\mathbb{E}[z] + \\mu + \\mathbb{E}[\\epsilon]$\n",
    "- $\\mathbb{E}[(Wz + \\epsilon)(Wz + \\epsilon)^T] = W \\mathbb{E}[zz^T] W^T + \\mathbb{E}[\\epsilon \\epsilon^T]$\n",
    "\n",
    "#### Posterior\n",
    "\n",
    "Using Bayes we can obtain the posterior to go from observed to latent\n",
    "\n",
    "$$\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-B), M\\sigma^{-2} )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M = W^T W + I\\sigma^2\n",
    "$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "We fit the model to find $W$, $\\mu$ and $\\sigma$ by maximizing the marginal likelihood\n",
    "\n",
    "$$\n",
    "\\max \\log L(W, B, \\sigma^2) = \\sum_{i=1}^N \\log p(x_i)\n",
    "$$\n",
    "\n",
    "From here we can do derivatives and obtain closed form solutions of the parameters\n",
    "\n",
    "> Solution for $W$ is equivalent to conventional PCA ($\\sigma^2 \\to 0$)\n",
    "\n",
    "> Now we have estimated $\\sigma$, we have error-bars for $z$ and the model is generative\n",
    "\n",
    "\n",
    "## Self-study\n",
    "- Barber, Chapter 21 and Murphy, Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import theano.tensor as tt\n",
    "from sklearn.decomposition import PCA\n",
    "N = 1000 \n",
    "M = 3  # dimensions of the data\n",
    "D = 2  # dimensions of the projection\n",
    "\n",
    "np.random.seed(10)\n",
    "C = np.random.randn(M, M)\n",
    "C = np.dot(C.T, C)\n",
    "X = np.random.multivariate_normal(np.zeros(shape=(M, )), C, size=N)\n",
    "X = X - np.mean(X, axis=0)\n",
    "X = X/np.std(X, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "R = pca.fit_transform(X)\n",
    "ax = fig.add_subplot(122)\n",
    "plt.scatter(R[:, 0], R[:, 1], s=1)\n",
    "_ = plt.title('PCA projection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as PPCA:\n",
    "    s = pm.HalfCauchy('s', beta=5, shape=[1,])\n",
    "    w = pm.Normal('w', mu=tt.zeros([D, M]), sd=tt.ones([D, M]), shape=[D, M])\n",
    "    z = pm.Normal('z', mu=tt.zeros([N, D]), sd=tt.ones([N, D]), shape=[N, D])\n",
    "    x = pm.Normal('x', mu=z.dot(w), sd=s*tt.ones([N, M]), shape=[N, M], observed=X)  \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=2000, method=inference, obj_optimizer=pm.adam(learning_rate=1e-1))\n",
    "\"\"\"\n",
    "_ = plt.plot(-inference.hist)\n",
    "plt.ylabel('Evidence lower bound (ELBO)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid()\n",
    "\"\"\"\n",
    "with PPCA:\n",
    "    trace = approx.sample(draws=1000)\n",
    "    ppc = pm.sample_ppc(trace=trace, samples=100)\n",
    "_ = pm.traceplot(trace=trace, varnames=['w', 's'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_avg = np.mean(trace['w'], axis=0)\n",
    "s_avg = np.mean(trace['s'], axis=0)\n",
    "print(\"Average W\")\n",
    "print(W_avg)\n",
    "print(\"Average sigma: %f\" %(s_avg))\n",
    "\n",
    "x_reconstructed = ppc['x'][0, :, :] \n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "ax.set_title('Input data')             \n",
    "bx, by, bz = ax.get_xbound(), ax.get_ybound(), ax.get_zbound()      \n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.set_title(\"Sampled data\")\n",
    "ax.scatter(x_reconstructed[:, 0], x_reconstructed[:, 1], x_reconstructed[:, 2], s=1, alpha=0.5)\n",
    "t = np.linspace(-4, 4, num=100)\n",
    "ax.set_xbound(bx)\n",
    "ax.set_ybound(by)\n",
    "ax.set_zbound(bz)\n",
    "\n",
    "z_trace_avg = np.mean(trace['z'], axis=0)\n",
    "z_trace_std = np.std(trace['z'], axis=0)\n",
    "z_trace_var = np.mean(np.var(trace['z'], axis=1), axis=0)\n",
    "# Sort the new axis in decreasing order of variance\n",
    "axis_order = np.argsort(z_trace_var)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3), tight_layout=True)\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.errorbar(z_trace_avg[:, axis_order[0]], z_trace_avg[:, axis_order[1]], \n",
    "            z_trace_std[:, axis_order[0]], z_trace_std[:, axis_order[1]], fmt='none', alpha= 0.5)\n",
    "plt.title('Average z from trace')\n",
    "\n",
    "Z_test = np.dot(X, np.dot(np.linalg.inv(np.dot(W_avg.T, W_avg) + np.eye(M)*s_avg**2 ), W_avg.T))\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.scatter(Z_test[:, axis_order[0]], Z_test[:, axis_order[1]], s=1, alpha=0.5)\n",
    "_ = plt.title('Average z by hand')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.scatter(R[:, 0], R[:, 1], s=1, alpha=0.5)\n",
    "_ = plt.title('z from sklearn PCA')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "# SKLEARN gives you the new axis already sorted by variance, also axis might appear rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "**Gaussian Mixture Model:** Model with categorical latent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generative model with latent variable $z$ y observed variable $x$ is described by the joint density\n",
    "\n",
    "$$\n",
    "p_\\theta(x, z) = p_\\theta(x|z) p(z)\n",
    "$$\n",
    "\n",
    "Tipically, we find the parameters $\\theta$ of our generator using a maximum likelihood approach over the **evidence** or **marginal likelihood** $p_\\theta (x)$\n",
    "\n",
    "$$\n",
    "\\max_\\theta p_\\theta (x) = \\int p_\\theta(x|z) p(z) \\,dz\n",
    "$$\n",
    "\n",
    "but the integral is in general intractable.\n",
    "\n",
    "Instead we will optimize a lower bound of the evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simpler posterior $q_\\phi(z|x)$ and write the divergence between this and the generator posterior\n",
    "\n",
    "$$\n",
    "p_\\theta(z|x) = \\frac{p_\\theta(x|z) p(z)}{p_\\theta(x)}, \n",
    "$$\n",
    "where $p(z)$ is a prior specified by the user\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_\\text{KL}\\left[q_\\phi(z|x) || p_\\theta(z|x)\\right] &=\n",
    "\\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)}\\right ]\\nonumber \\\\\n",
    "&= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ \\log \\frac{p_\\theta(x)}{p_\\theta(x|z)}\\frac{q_\\phi(z|x)}{p(z)} \\right ] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) + \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ - \\log p_\\theta(x|z) + \\log \\frac{q_\\phi(z|x)}{p(z)} \\right ] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) - \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z)\\right ] + D_\\text{KL}\\left[q_\\phi(z|x) || p(z)\\right] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) - \\mathcal{L}_{\\theta, \\phi} (x) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From here we can do\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta(x) &= \\mathcal{L}_{\\theta, \\phi} (x) + D_\\text{KL}\\left[q_\\phi(z|x) || p_\\theta(z|x)\\right] \\nonumber \\\\\n",
    "&\\geq \\mathcal{L}_{\\theta, \\phi} (x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "because the KL divergence is non-negative\n",
    "\n",
    "The term\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\theta, \\phi} (x) = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z)\\right ] + D_\\text{KL}\\left[q_\\phi(z|x) || p(z)\\right],\n",
    "$$\n",
    "\n",
    "is known as the *Evidence Lower Bound* (ELBO)\n",
    "\n",
    "\n",
    "El ELBO se puede obtener de forma alternativa aplicando la desigualdad de Jensen sobre\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x) = \\log \\mathbb{E}_{z\\sim p(z)}[p_\\theta(x|z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de Gradiente descedente con ADAM para entrenar\n",
    "mi_red_neuronal = RedEjemplo()\n",
    "optimizer = torch.optim.Adam(mi_red_neuronal.parameters(), lr=1e-3)\n",
    "\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "# GPU: Para entrenar en GPU trasladamos el modelo con\n",
    "#mi_red_neuronal = mi_red_neuronal.cuda()\n",
    "#for epoch in tqdm_notebook(range(10)):\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    KL_loss = 0.0\n",
    "    for image, label in train_loader:\n",
    "        # GPU: También trasladamos las imágenes\n",
    "        #image = image.cuda()\n",
    "        #label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction, mu, logsigma = mi_red_neuronal.forward(image)\n",
    "        rec, KLdiv = ELBO(image, prediction, mu, logsigma)\n",
    "        loss = rec + KLdiv\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += rec.item()  \n",
    "        KL_loss += KLdiv.item()  \n",
    "    den = train_loader.__len__()*train_loader.batch_size\n",
    "    history1.log(epoch, loss_train=epoch_loss/den, kl_train=KL_loss/den)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    KL_loss = 0.0\n",
    "    for image, label in valid_loader:\n",
    "        prediction, mu, logsigma = mi_red_neuronal.forward(image)\n",
    "        rec, KLdiv = ELBO(image, prediction, mu, logsigma)\n",
    "        epoch_loss += rec.item()  \n",
    "        KL_loss += KLdiv.item()  \n",
    "    den = valid_loader.__len__()*valid_loader.batch_size\n",
    "    history1.log(epoch, loss_valid=epoch_loss/den, kl_valid=KL_loss/den)\n",
    "\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss_train\"], history1[\"loss_valid\"]],\n",
    "                          labels=[\"Train loss\", \"Validation loss\"])\n",
    "        canvas1.draw_plot([history1[\"kl_train\"], history1[\"kl_valid\"]],\n",
    "                          labels=[\"Train KL\", \"Validation KL\"])\n",
    "        #canvas1.draw_plot([history1[\"loss_valid\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
