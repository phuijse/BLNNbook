**Paper reading**

Select a topic and work in groups to review the papers. Papers will be presented in class

1. Capacidad de aproximación de redes profundas y Teoremas de aproximación universal
    1. [Liang, S. and Srikant, R., 2016.](https://arxiv.org/abs/1610.04161) Why deep neural networks for function approximation?, In International Conference on Learning Representations (ICLR)
    1. [Lu, Z., Pu, H., Wang, F., Hu, Z. and Wang, L., 2017.](https://proceedings.neurips.cc/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html) The expressive power of neural networks: a view from the width. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6232-6240).
    1. [Hanin, B., 2019](https://www.mdpi.com/2227-7390/7/10/992). Universal function approximation by deep neural nets with bounded width and relu activations. Mathematics, 7(10), p.992.

1. Generalización, memorización y calibración de modelos profundos
    1. [Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O., 2016](https://openreview.net/forum?id=Sy8gdB9xx). Understanding deep learning requires rethinking generalization, In International Conference on Learning Representations (ICLR)
    1. [Arpit, D., Jastrzębski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y. and Lacoste-Julien, S., 2017](http://proceedings.mlr.press/v70/arpit17a). A closer look at memorization in deep networks. In International Conference on Machine Learning (pp. 233-242). PMLR.
    1. [Guo, C., Pleiss, G., Sun, Y. and Weinberger, K.Q., 2017.](http://proceedings.mlr.press/v70/guo17a.html) On calibration of modern neural networks. In International Conference on Machine Learning (pp. 1321-1330). PMLR.

1. Aprendizaje con  modelos sobre-parametrizados
    1. [Du, S.S., Zhai, X., Poczos, B. and Singh, A., 2018.](https://openreview.net/forum?id=BygfghAcYX) Gradient Descent Provably Optimizes Over-parameterized Neural Networks. In International Conference on Learning Representations (ICLR).
    1. [Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y. and Srebro, N., 2018.](https://openreview.net/forum?id=BygfghAcYX) The role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations.
    1. [Allen-Zhu, Z., Li, Y. and Liang, Y., 2019](https://proceedings.neurips.cc/paper/2019/hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html). Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers. Advances in neural information processing systems.
    
1. La hipótesis del boleto de loteria: Inicialización aleatoria y modelos profundos
    1. [Frankle, J. and Carbin, M., 2018.](https://openreview.net/forum?id=rJl-b3RcF7). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In International Conference on Learning Representations.
    1. [Malach, E., Yehudai, G., Shalev-Schwartz, S. and Shamir, O., 2020](http://proceedings.mlr.press/v119/malach20a.html). Proving the lottery ticket hypothesis: Pruning is all you need. In International Conference on Machine Learning (pp. 6682-6691). PMLR.
    1. [Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A. and Rastegari, M., 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.html). What's Hidden in a Randomly Weighted Neural Network?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11893-11902).