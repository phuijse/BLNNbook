{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many interesting models (neural networks) the evidence  \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) = \\int p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i) d\\theta\n",
    "$$\n",
    "\n",
    "is intractable:\n",
    "\n",
    "- The integral has no closed-form\n",
    "- The dimensionality is so big that numerical integration is not feasible\n",
    "\n",
    "If the evidence is intractable then the posterior is also intractable\n",
    "\n",
    "In these cases we resort to stochastic or deterministic approximations\n",
    "\n",
    "- **Stochastic:** For example Markov Chain Monte Carlo, which is computationally demanding (for complex models) but produces asymptotically exact samples from the intractable distribution\n",
    "- **Deterministic:** For example Variational Inference, which scales better than MCMC but it is not exact. Instead of samples we get a direct approximation of the intractable distribution\n",
    "\n",
    "The main topic of this lecture is deterministic approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Approximation\n",
    "\n",
    "In this deterministic approximation we first propose a function of the parameter vector $\\theta \\in \\mathbb{R}^K$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\theta) &= \\log  p(\\mathcal{D}| \\theta) p(\\theta) \\nonumber \\\\\n",
    "&= \\log  p(\\mathcal{D}| \\theta) + \\log   p(\\theta) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log p(x_i |\\theta) + \\log   p(\\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and then perform a second order Taylor expansion around $\\theta= \\hat \\theta_{\\text{map}}$ (the MAP solution)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) &+ (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{dg}{d\\theta}\\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} \\nonumber \\\\\n",
    "&+ \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{d^2 g}{d\\theta^2} \\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} (\\theta - \\hat \\theta_{\\text{map}})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by definition the first derivative evaluated at $\\hat \\theta_{\\text{map}}$ is zero. We will call the negative Hessian evaluated at $\\hat \\theta_{\\text{map}}$: \n",
    "\n",
    "$$\n",
    "\\Lambda = -\\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}})\n",
    "$$ \n",
    "\n",
    "Then we can rewrite the approximation as\n",
    "\n",
    "$$\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) - \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})\n",
    "$$\n",
    "\n",
    "> We have \"Gaussianized\" the approximation, where $\\Lambda$ is the precision matrix (inverse covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plug the \"Gaussianized\" approximation in the evidence we can solve the integral\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) \\approx  e^{g(\\hat \\theta_{\\text{map}})} \\int e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})} d\\theta = e^{g(\\hat \\theta_{\\text{map}})} (2\\pi)^{K/2} |\\Lambda|^{-1/2}\n",
    "$$\n",
    "\n",
    "and also the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\mathcal{D}) &= \\frac{p(\\mathcal{D}|\\theta) p(\\theta) }{p(\\mathcal{D})} \\nonumber \\\\\n",
    "&\\approx \\frac{1}{(2\\pi)^{K/2} |\\Lambda|^{-1/2}} e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Laplace's method approximates the posterior by a **Multivariate Gaussian** centered on the MAP. \n",
    "\n",
    "We should ask ourselves if the Gaussian is adequate to approximate the distribution we are interested. Also note that Laplace's method might not be the \"best\" gaussian fit to our distribution\n",
    "\n",
    "<img src=\"images/laplace_vs_vi.png\" width=\"400\">\n",
    "\n",
    "The picture shows a non-gaussian posterior in yellow. The red line corresponds to a gaussian centered on the mode (Laplace), while the green line corresponds to a gaussian with minimum reverse KL divergence to the posterior \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement of the Laplace approximation**\n",
    "\n",
    "1. The MAP solution and the negative Hessian evaluated at the MAP solution\n",
    "1. For the above $g$ should be continuous and differentiable on $\\theta$, and the negative Hessian has to be positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A closer look to the evidence in Laplace approximation**\n",
    "\n",
    "Using Laplace approximation the log evidence can be decomposed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) &\\approx g(\\hat \\theta_{\\text{map}}) + \\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log | \\Lambda | \\nonumber \\\\\n",
    "&=\\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{map}}) + \\log p(\\hat \\theta_{\\text{map}}| \\mathcal{M}_i) + \\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log | \\Lambda | \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The log evidence is approximated by the best likelihood fit plus the **Occam's factor**\n",
    "\n",
    "The Occam's factor depends on the\n",
    "\n",
    "- log pdf of $\\theta$: Prior\n",
    "- number of parameters $K$: Complexity\n",
    "- second derivative around the MAP: Model uncertainty\n",
    "\n",
    "If we have a very broad prior, the approximation further reduces to \n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) \\approx \\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{mle}})  + \\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log | \\Lambda |, \n",
    "$$\n",
    "\n",
    "where $\\theta_{\\text{mle}}$ is the maximum likelihood solution\n",
    "\n",
    "We can further reduce the previous expression by assuming that lambda grows linearly with the number of samples: $\\Lambda \\approx N \\Lambda_0$ and then considering the large $N$ regime\n",
    "\n",
    "$$\n",
    "\\text{negBIC} = \\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{mle}}) - \\frac{K}{2} \\log N,\n",
    "$$\n",
    "\n",
    "which gives us the negative of the Bayesian Information Criterion (BIC). In previous courses we minimized the BIC for model selection. Note all the assumptions hidden within BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference \n",
    "\n",
    "Now we will review a more general method for deterministic approximation. Remember that we want the (intractable) posterior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "Variational Inference (VI) is a family of methods in which a simpler (tractable) posterior distribution is proposed to \"replace\" $p(\\theta|\\mathcal{D})$. This simpler posterior is denoted as $q_\\nu(\\theta)$ which represents a family of distributions parameterized by $\\nu$\n",
    "\n",
    "> **Optimization problem:** The objective is to find $\\nu$ that makes $q$ most similar to $p$\n",
    "\n",
    "<img src=\"images/vi.png\" width=\"350\">\n",
    "\n",
    "We can formalize this as a KL divergence minimization problem\n",
    "\n",
    "$$\n",
    "\\hat \\nu =  \\text{arg}\\min_\\nu D_{\\text{KL}}[q_\\nu(\\theta) || p(\\theta|\\mathcal{D})] = \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\theta|\\mathcal{D})} d\\theta,\n",
    "$$\n",
    "\n",
    "but this expression depends on the intractable posterior. To continue we use Bayes Theorem to move the evidence out from the integral\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}[q_\\nu(\\theta) || p(\\theta|\\mathcal{D})] = \\log p(\\mathcal{D}) + \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}, \\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "As $p(\\mathcal{D})$ does not depend on $\\nu$ we can focus on the right hand term. The optimization problem is typically written as \n",
    "\n",
    "$$\n",
    "\\hat \\nu =  \\text{arg}\\max_\\nu \\mathcal{L}(\\nu) =\\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}, \\theta)}{q_\\nu(\\theta)}d\\theta\n",
    "$$\n",
    "\n",
    "where $ \\mathcal{L}(\\nu)$ is called the **Evidence Lower BOund** (ELBO). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name comes from the fact that \n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\nu) = \\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}, \\theta)}{q_\\nu(\\theta)}d\\theta,\n",
    "$$\n",
    "\n",
    "which is a result of the non-negativity of the KL divergence.\n",
    "\n",
    "> Ideally we choose a \"simple-enough\" parametric family $q$ so that the ELBO is tractable. \n",
    "\n",
    "> The ELBO can only be tight if $p$ is within the family of $q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿Why is it called variational?\n",
    "\n",
    "- Functional: Function of functions. [Calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations): Derivatives of functionals\n",
    "- [Variational Free Energy](https://en.wikipedia.org/wiki/Thermodynamic_free_energy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More attention on the ELBO \n",
    "\n",
    "The ELBO can also be decomposed as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\nu) &= \\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}|\\theta) p (\\theta)}{q_\\nu(\\theta)}d\\theta \\nonumber \\\\\n",
    "&= \\int q_\\nu(\\theta) \\log p(\\mathcal{D}|\\theta) d\\theta - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{ p (\\theta)} d\\theta  \\nonumber \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right] - D_{KL}[q_\\nu(\\theta) || p(\\theta)]  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From which we can recognize that maximizing the ELBO is equivalent to:\n",
    "\n",
    "- Maximizing the log likelihood for parameters sampled from the approximate posterior: Generative model produces realistic data samples\n",
    "- Minimizing the KL divergence between the approximate posterior and prior: Regularization for the approximate posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to \"obtain\" the ELBO\n",
    "\n",
    "We can get the ELBO using [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) on the log evidence \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}) &=  \\log \\mathbb{E}_{\\theta\\sim p(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\right]\\nonumber \\\\\n",
    "&=  \\log \\mathbb{E}_{\\theta\\sim q_\\nu(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\nu(\\theta)}\\right]\\nonumber \\\\\n",
    "&\\geq  \\mathbb{E}_{\\theta\\sim q_\\nu(\\theta)} \\left[\\log \\frac{p(\\mathcal{D},\\theta)}{q_\\nu(\\theta)}\\right] =\\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D},\\theta)}{q_\\nu(\\theta)}d\\theta \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple posterior: Fully-factorized posterior\n",
    "\n",
    "A broadly-used idea to make posteriors tractable is to assume that there is no correlation between factors \n",
    "\n",
    "$$\n",
    "q_\\nu(\\theta) = \\prod_{i=1}^K q_{\\nu}(\\theta_i), \n",
    "$$\n",
    "\n",
    "this is known as the Mean-field VI or Mean-field Theory (physics)\n",
    "\n",
    "Replacing this factorized posterior on the ELBO\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_\\text{MF}(\\nu) &= \\int  \\prod_{i=1}^K q_{\\nu}(\\theta_i) \\left ( \\log p(\\mathcal{D}, \\theta) - \\sum_{i=1}^K \\log q_{\\nu}(\\theta_i) \\right) d\\theta  \\nonumber \\\\\n",
    "&= \\int q_{\\nu_i}(\\theta_i) \\left [ \\prod_{j\\neq i} q_{\\nu_j}(\\theta_j) \\log p(\\mathcal{D}, \\theta) d\\theta_j \\right ] d\\theta_i - \\sum_{i=1}^K \\int q_{\\nu_i}(\\theta_i) \\log q_{\\nu_i}(\\theta_i)  d\\theta_i \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumming that we keep all $\\theta$ except $\\theta_i$ fixed, we can update $\\theta_i$ iteratively using\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{MF}(\\nu_i) =  \\int q_{\\nu_i}(\\theta_i) \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D},\\theta) \\right ] d\\theta_i - \\int q_{\\nu_i}(\\theta_i) \\log q_{\\nu_i}(\\theta_i)  d\\theta_i + \\text{Constant}  \n",
    "$$\n",
    "\n",
    "> (in this case) Maximizing the ELBO is equivalent to minimizing the KL between $q_{\\nu_i}(\\theta_i)$ and $\\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}, \\theta) \\right ]$\n",
    "\n",
    "The optimal $q$ in this case is given by\n",
    "\n",
    "$$\n",
    "q_i(\\nu_i) \\propto  \\exp \\left ( \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}, \\theta) \\right ] \\right )\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study\n",
    "\n",
    "- [Chapter 28 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "    - 28.2 Laplace Approximation\n",
    "    - 28.4 Variational Bounding\n",
    "- David Blei, [\"Variational Inference: A review for statisticians\"](https://arxiv.org/abs/1601.00670), [\"Foundations and innovations\"](https://www.youtube.com/watch?v=DaqNNLidswA)\n",
    "- Tamara Broderick, [\"Variational Bayes and beyond: Bayesian inference for big data\"](http://www.tamarabroderick.com/tutorial_2018_icml.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
