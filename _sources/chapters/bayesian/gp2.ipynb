{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes with `pyro`\n",
    "\n",
    "### Setting, training and performing inference \n",
    "\n",
    "Let's start by creating some synthetic data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) #100x1\n",
    "x_test = np.linspace(-0.4, 1.6, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [`pyro.contrib.gp`](http://docs.pyro.ai/en/stable/contrib.gp.html) to implement our first GP\n",
    "\n",
    "Let's start by creating a kernel from `gp.kernels`\n",
    "\n",
    "We will use a Radial Basis Function (RBF) aka Squared Exponential aka Gaussian kernel as our covariance\n",
    "\n",
    "We can specify the initial value of the variance and the lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.), \n",
    "                   lengthscale=torch.tensor(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model looks before fitting the data? \n",
    "\n",
    "Let's inspect the prior $p(f) = \\mathcal{N}(0, K)$ on the test data\n",
    "\n",
    "**Activity:** Increase/decrese the lengthscale and repeat, get a notion of its influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "C = K.forward(torch.from_numpy(x_test)) + torch.eye(len(x_test))*1e-4\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(torch.zeros(len(x_test)), \n",
    "                                                covariance_matrix=C).sample(sample_shape=(20,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :],\n",
    "            linestyle='-', c='tab:blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train GP and plot the results\n",
    "\n",
    "def train_gp_plots(model, x, y, x_test, nepochs=2000):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "    line_loss = ax[1].plot([], [])\n",
    "    ax[0].scatter(x, y)\n",
    "    epoch_loss = np.zeros(shape=(nepochs,))\n",
    "\n",
    "    for k in tqdm(range(len(epoch_loss))):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model.model, model.guide)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss[k] = loss.item()\n",
    "        #break    \n",
    "        if k % 100 == 0:\n",
    "            ax[0].cla()\n",
    "            # Predictions at x_test\n",
    "            mu, cov = model.forward(x_test, full_cov=True, noiseless=False)\n",
    "            mu = mu.detach().numpy()\n",
    "            sd = cov.diag().sqrt().detach().numpy()        \n",
    "            ax[0].scatter(x, y, c='k')\n",
    "            ax[0].plot(x_test.detach(), mu)\n",
    "            ax[0].fill_between(x_test.detach(), mu-2*sd, mu+2*sd, alpha=0.5)\n",
    "            line_loss[0].set_xdata(range(k))\n",
    "            line_loss[0].set_ydata(epoch_loss[:k])\n",
    "            ax[1].relim()\n",
    "            ax[1].autoscale_view()\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model from `gp.models`. For regression there is `GPRegression` and for classification (non-gaussian likelihoods) we can use `VariationalGP`. There are also more efficient (sparse) versions of both models\n",
    "\n",
    "The model expects \n",
    "\n",
    "- the train data\n",
    "- the kernel \n",
    "- initial value of the noise variance\n",
    "\n",
    "We may also specify a mean function for the GP\n",
    "\n",
    "To train the model we have to select an optimizer and a cost function. We will use Adam and the Trace_ELBO, respectively\n",
    "\n",
    "Training is very similar to how we train neural networks in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.01))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, # Training data\n",
    "                                   mean_function=None, # Mean\n",
    "                                   kernel=K, # Covarianze\n",
    "                                   jitter=1e-6, # Increase this if you have numerical problems \n",
    "                                   noise=torch.tensor(2.) # The variance of the white noise\n",
    "                                   )\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"RBF variance:\", gpr_model.kernel.variance.item())\n",
    "display(\"RBF length scale:\", gpr_model.kernel.lengthscale.item())\n",
    "display(\"Noise variance:\", gpr_model.noise.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do predictions we use the forward attribute of our `GPRegression` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "mu, Sigma = gpr_model.forward(torch.from_numpy(x_test), full_cov=True, noiseless=True)\n",
    "Sigma += torch.eye(len(x_test))*1e-5\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(mu, covariance_matrix=Sigma).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :], \n",
    "            linestyle='-', c='tab:blue', alpha=0.25)\n",
    "ax.scatter(x, y, c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different kernels\n",
    "\n",
    "Kernel are implemented in [pyro.contrib.gp.kernels](http://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.kernels)\n",
    "\n",
    "Compare the RBF and Matern52 kernels. What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "#K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "K = gp.kernels.Matern52(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "#K = gp.kernels.Periodic(input_dim=1, variance=torch.tensor(2.0), \n",
    "#                        lengthscale=torch.tensor(0.1), \n",
    "#                        period=torch.tensor(2.))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting priors for the parameters \n",
    "\n",
    "Before we did an MLE-like estimation to find point estimates of the kernel hyper-parameters and the noise variance\n",
    "\n",
    "We can \"go bayesian\" and treat these parameters as random variables and set priors for them\n",
    "\n",
    "Training with these priors is equivalent to the MAP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "from pyro.distributions import LogNormal\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model_prior = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "\n",
    "# Setting priors\n",
    "gpr_model_prior.kernel.lengthscale = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "gpr_model_prior.kernel.variance = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model_prior.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot    \n",
    "train_gp_plots(gpr_model_prior, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining kernels\n",
    "\n",
    "Adding or multiplying valid kernels yield a valid kernel\n",
    "\n",
    "> We can easily create new kernels from other kernels\n",
    "\n",
    "and take advantage of their different properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data has a periodic oscilation on a rising trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100).astype('float32')*100\n",
    "y = (0.03*x + np.sin(0.1*x) + 0.1*np.random.randn(100)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fit it using `K1`, `K2`, `Ksum12` and `Kprod12`\n",
    "\n",
    "Can you explain in simple words your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "K1 = gp.kernels.Periodic(input_dim=1, variance=torch.tensor(1.), \n",
    "                        lengthscale=torch.tensor(10),\n",
    "                        period=torch.tensor(50))\n",
    "K2 = gp.kernels.Linear(input_dim=1, variance=torch.tensor(1.))\n",
    "Ksum12 = gp.kernels.Sum(K1, K2)\n",
    "Kprod12 = gp.kernels.Product(K1, K2)\n",
    "\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(torch.from_numpy(x), torch.from_numpy(y), \n",
    "                                   kernel=K1, noise=torch.tensor(2.))\n",
    "\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "train_gp_plots(gpr_model, x, y, torch.linspace(-50, 150, 100), nepochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Gaussian Processes with `pyro`\n",
    "\n",
    "Fitting a Gaussian process has cubic complexity\n",
    "\n",
    "Sparse Gaussian processes use a much smaller set of \"inducing points\" to compute the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=1000) #100x1\n",
    "x_test = np.linspace(-0.1, 1.1, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "#gpr_model = gp.models.SparseGPRegression(x_torch, y_torch, approx='VFE',\n",
    "#                                         kernel=K, Xu=torch.linspace(0, 1, 20),\n",
    "#                                         noise=torch.tensor(2.), jitter=1e-5)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics for the future\n",
    "\n",
    "- Example of GP for classification (Bernoulli likelihood), Barber 19.5\n",
    "- Deep Gaussian Processes/Stacks of Gaussian Processes. Example in Pyro: https://fehiepsi.github.io/blog/deep-gaussian-process/\n",
    "- [Compositional kernel learning](https://arxiv.org/pdf/1806.04326.pdf)\n",
    "- Bayesian optimization with Gaussian processes\n",
    "- https://github.com/team-approx-bayes/dnn2gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-study\n",
    "\n",
    "- Mackay's book, chapter 45 on Gaussian Proceses\n",
    "- Barber's book, chapter 19 on Gaussian Processes\n",
    "- [Rasmussen & Willams, \"Gaussian Process for Machine Learning\"](http://gaussianprocess.org/gpml/?)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
