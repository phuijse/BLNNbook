{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many interesting models (e.g. neural networks) the evidence  \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) = \\int p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i) d\\theta\n",
    "$$\n",
    "\n",
    "is intractable, *i.e* either the integral has no closed-form or the dimensionality is so big that numerical integration is not feasible\n",
    "\n",
    "> If the evidence is intractable then the posterior is also intractable\n",
    "\n",
    "In these cases we resort to approximations\n",
    "\n",
    "- **Stochastic approximation:** For example Markov Chain Monte Carlo (MCMC). MCMC is computationally demanding (for complex models) but produces asymptotically exact samples from the intractable distribution\n",
    "- **Deterministic approximation:** For example Variational Inference (VI). VI is more efficient than MCMC, but it is not asymptotically exact. Instead of samples we get a direct approximation of the intractable distribution\n",
    "\n",
    "The main topic of this lecture is deterministic approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Approximation\n",
    "\n",
    "In Bayesian statistics, the Laplace approximation refers to the application of [Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)'s method to approximate an intractable integral (evidence) using a Gaussian distribution\n",
    "\n",
    "In particular, Laplace's method is a technique to solve integrals of the form\n",
    "\n",
    "$$\n",
    "f(x) = \\int e^{g(\\theta)} d\\theta,\n",
    "$$\n",
    "\n",
    "by defining the auxiliary function as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "g(\\theta) &= \\log  p(\\mathcal{D}| \\theta) p(\\theta)  \\\\\n",
    "&= \\log  p(\\mathcal{D}| \\theta) + \\log   p(\\theta)  \\\\\n",
    "&= \\sum_{i=1}^N \\log p(x_i |\\theta) + \\log   p(\\theta) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "then $f(x)$ is equivalent to the evidence.\n",
    "\n",
    "The \"approximation\" consists of performing a second order Taylor expansion of $g(\\theta)$ around $\\theta= \\hat \\theta_{\\text{map}}$, i.e. the MAP solution. The result of this is\n",
    "\n",
    "$$\n",
    "g(\\theta) \\approx  g(\\hat \\theta_{\\text{map}}) -  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Lambda = -\\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}}),\n",
    "$$ \n",
    "\n",
    "is the negative Hessian evaluated at $\\hat \\theta_{\\text{map}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "By definition the first derivative of $g(\\theta)$ evaluated at $\\hat \\theta_{\\text{map}}$ is zero\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plug the Gaussian approximation back into the evidence we can now solve the integral as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(\\mathcal{D}) &\\approx  e^{g(\\hat \\theta_{\\text{map}})} \\int e^{-\\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})} d\\theta  \\\\\n",
    "&= e^{g(\\hat \\theta_{\\text{map}})} (2\\pi)^{K/2} |\\Lambda|^{-1/2}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $K$ is the dimensionality of $\\theta$. With this the posterior is\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(\\theta| \\mathcal{D}) &= \\frac{p(\\mathcal{D}|\\theta) p(\\theta) }{p(\\mathcal{D})}  = \\frac{e^{ g(\\theta)}}{\\int e^{ g(\\theta)} d\\theta} = \\\\\n",
    "&\\approx \\frac{1}{(2\\pi)^{K/2} |\\Lambda|^{-1/2}} e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Lambda (\\theta - \\hat \\theta_{\\text{map}})} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Laplace's method approximates the posterior by a **Multivariate Gaussian** centered on the MAP. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{warning}\n",
    "\n",
    "As the following figure shows, Laplace's method might not be the \"best\" gaussian fit to our distribution\n",
    "\n",
    ":::\n",
    "\n",
    "<img src=\"images/laplace_vs_vi.png\" width=\"400\">\n",
    "\n",
    "A non-gaussian distribution is shown in yellow. The red line corresponds to a gaussian centered on the mode (Laplace approximation), while the green line corresponds to a gaussian with minimum reverse KL divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement of the Laplace approximation**\n",
    "\n",
    "For Laplace's approximation the MAP solution and the negative Hessian evaluated at the MAP solution are needed\n",
    "\n",
    "In the first place $g(\\theta)$ has to be continuous and differentiable on $\\theta$, and the negative Hessian has to be positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A closer look to the evidence in Laplace approximation**\n",
    "\n",
    "Using Laplace approximation the log evidence can be decomposed as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) &\\approx g(\\hat \\theta_{\\text{map}}) + \\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log | \\Lambda |  \\\\\n",
    "&=\\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{map}}) + \\log p(\\hat \\theta_{\\text{map}}| \\mathcal{M}_i) + \\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log | \\Lambda |  \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "*i.e.* the log evidence is approximated by the best likelihood fit plus the **Occam's factor**\n",
    "\n",
    "The Occam's factor depends on the\n",
    "\n",
    "- log pdf of $\\theta$: Prior\n",
    "- number of parameters $K$: Complexity\n",
    "- second derivative around the MAP: Model uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship between the evidence in Laplace approximation and the BIC**\n",
    "\n",
    "In the regime of very large number of samples ($N$) it can be shown that Laplace's approximations is dominated by\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) \\approx \\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{mle}}) - \\frac{K}{2} \\log N, \n",
    "$$\n",
    "\n",
    "where $\\theta_{\\text{mle}}$ is the maximum likelihood solution.\n",
    "\n",
    "The expression above is equivalent to the negative of the [Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
    "\n",
    ":::{seealso}\n",
    "\n",
    "- Chapters 27 and 28 of [D. Mackay's book](http://www.inference.org.uk/itprnn/book.pdf)\n",
    "- Section 28.2 of [D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference \n",
    "\n",
    "In this section we review a more general method for deterministic approximation. Remember that we are interested in the (intractable) posterior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "Variational Inference (VI) is a family of methods in which a simpler (tractable) posterior distribution is proposed to \"replace\" $p(\\theta|\\mathcal{D})$. This simpler posterior is denoted as $q_\\nu(\\theta)$ which represents a family of distributions parameterized by $\\nu$\n",
    "\n",
    "> **Optimization problem:** The objective is to find $\\nu$ that makes $q$ most similar to $p$\n",
    "\n",
    "<img src=\"images/vi.png\" width=\"350\">\n",
    "\n",
    "We can formalize this as a KL divergence minimization problem\n",
    "\n",
    "$$\n",
    "\\hat \\nu =  \\text{arg}\\min_\\nu D_{\\text{KL}}[q_\\nu(\\theta) || p(\\theta|\\mathcal{D})] = \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\theta|\\mathcal{D})} d\\theta,\n",
    "$$\n",
    "\n",
    "but this expression depends on the intractable posterior. To continue we use Bayes Theorem to move the evidence out from the integral\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}[q_\\nu(\\theta) || p(\\theta|\\mathcal{D})] = \\log p(\\mathcal{D}) + \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}, \\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "As $p(\\mathcal{D})$ does not depend on $\\nu$ we can focus on the right hand term. The optimization problem is typically written as \n",
    "\n",
    "$$\n",
    "\\hat \\nu =  \\text{arg}\\max_\\nu \\mathcal{L}(\\nu) =\\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}, \\theta)}{q_\\nu(\\theta)}d\\theta\n",
    "$$\n",
    "\n",
    "where $ \\mathcal{L}(\\nu)$ is called the **Evidence Lower BOund** (ELBO). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name comes from the fact that \n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\nu) = \\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}, \\theta)}{q_\\nu(\\theta)}d\\theta,\n",
    "$$\n",
    "\n",
    "which is a result of the non-negativity of the KL divergence.\n",
    "\n",
    "> Ideally we choose a \"simple-enough\" parametric family $q$ so that the ELBO is tractable\n",
    "\n",
    ":::{note}\n",
    "\n",
    "The ELBO can only be tight if $p$ is within the family of $q$\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "[Calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations): Derivatives of functionals (function of functions)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More attention on the ELBO \n",
    "\n",
    "The ELBO can also be decomposed as \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\nu) &= \\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D}|\\theta) p (\\theta)}{q_\\nu(\\theta)}d\\theta  \\\\\n",
    "&= \\int q_\\nu(\\theta) \\log p(\\mathcal{D}|\\theta) d\\theta - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{ p (\\theta)} d\\theta   \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right] - D_{KL}[q_\\nu(\\theta) || p(\\theta)]   \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "From which we can recognize that maximizing the ELBO is equivalent to:\n",
    "\n",
    "- Maximizing the log likelihood for parameters sampled from the approximate posterior: Generative model produces realistic data samples\n",
    "- Minimizing the KL divergence between the approximate posterior and prior: Regularization for the approximate posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another way to \"obtain\" the ELBO**\n",
    "\n",
    "We can get the ELBO using [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) on the log evidence \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\log p(\\mathcal{D}) &=  \\log \\mathbb{E}_{\\theta\\sim p(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\right]\\nonumber \\\\\n",
    "&=  \\log \\mathbb{E}_{\\theta\\sim q_\\nu(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\nu(\\theta)}\\right] \\\\\n",
    "&\\geq  \\mathbb{E}_{\\theta\\sim q_\\nu(\\theta)} \\left[\\log \\frac{p(\\mathcal{D},\\theta)}{q_\\nu(\\theta)}\\right] =\\int q_\\nu(\\theta) \\log \\frac{p(\\mathcal{D},\\theta)}{q_\\nu(\\theta)}d\\theta  \n",
    "\\end{split}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple posterior: Fully-factorized posterior\n",
    "\n",
    "A broadly-used idea to make posteriors tractable is to assume that there is no correlation between factors \n",
    "\n",
    "$$\n",
    "q_\\nu(\\theta) = \\prod_{i=1}^K q_{\\nu}(\\theta_i), \n",
    "$$\n",
    "\n",
    "this is known as the **Mean-field** VI\n",
    "\n",
    "\n",
    "Replacing this factorized posterior on the ELBO\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}_\\text{MF}(\\nu) &= \\int  \\prod_{i=1}^K q_{\\nu}(\\theta_i) \\left ( \\log p(\\mathcal{D}, \\theta) - \\sum_{i=1}^K \\log q_{\\nu}(\\theta_i) \\right) d\\theta   \\\\\n",
    "&= \\int q_{\\nu_i}(\\theta_i) \\left [ \\int \\prod_{j\\neq i} q_{\\nu_j}(\\theta_j) \\log p(\\mathcal{D}, \\theta) d\\theta_j \\right ] d\\theta_i - \\sum_{i=1}^K \\int q_{\\nu_i}(\\theta_i) \\log q_{\\nu_i}(\\theta_i)  d\\theta_i  \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumming that we keep all $\\theta$ except $\\theta_i$ fixed, we can update $\\theta_i$ iteratively using\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{MF}(\\nu_i) =  \\int q_{\\nu_i}(\\theta_i) \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ \\log p(\\mathcal{D},\\theta) \\right ] d\\theta_i - \\int q_{\\nu_i}(\\theta_i) \\log q_{\\nu_i}(\\theta_i)  d\\theta_i + \\text{Constant}  \n",
    "$$\n",
    "\n",
    "\n",
    ":::{note}\n",
    "\n",
    "(In this case) Maximizing the ELBO is equivalent to minimizing the KL between $q_{\\nu_i}(\\theta_i)$ and $\\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}, \\theta) \\right ]$\n",
    "\n",
    ":::\n",
    "\n",
    "The optimal $q$ in this case is given by\n",
    "\n",
    "$$\n",
    "q_i(\\nu_i) \\propto  \\exp \\left ( \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}, \\theta) \\right ] \\right )\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "- Chapters 33 of [D. Mackay's book](http://www.inference.org.uk/itprnn/book.pdf)\n",
    "- Section 28.4 of [D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "- David Blei, [\"Variational Inference: A review for statisticians\"](https://arxiv.org/abs/1601.00670), [\"Foundations and innovations\"](https://www.youtube.com/watch?v=DaqNNLidswA)\n",
    "- Tamara Broderick, [\"Variational Bayes and beyond: Bayesian inference for big data\"](http://www.tamarabroderick.com/tutorial_2018_icml.html)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
